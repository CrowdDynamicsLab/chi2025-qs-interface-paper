@misc{AccessibleVotingCHI,
  title = {Accessible Voting {\textbar} {{CHI}} '11 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1979742.1979549},
  file = {C:\Users\tcheng\Zotero\storage\8LQUWABF\1979742.html}
}

@article{alwinMeasurementValuesSurveys1985,
  title = {The Measurement of Values in Surveys: {{A}} Comparison of Ratings and Rankings},
  author = {Alwin, Duane F and Krosnick, Jon A},
  year = {1985},
  journal = {Public Opinion Quarterly},
  volume = {49},
  number = {4},
  pages = {535--552},
  publisher = {Oxford University Press},
  file = {C:\Users\tcheng\Zotero\storage\LJ9Y7Q7P\Alwin and Krosnick - 1985 - The Measurement of Values in Surveys A Comparison.pdf}
}

@article{andreTooMuchChoice2017,
  title = {Too Much Choice, Too Little Impact: A Multilevel Analysis of the Contextual Determinants of Preference Voting},
  shorttitle = {Too Much Choice, Too Little Impact},
  author = {Andr{\'e}, Audrey and Depauw, Sam},
  year = {2017},
  month = may,
  journal = {West European Politics},
  volume = {40},
  number = {3},
  pages = {598--620},
  issn = {0140-2382, 1743-9655},
  doi = {10.1080/01402382.2016.1271596},
  urldate = {2022-05-30},
  abstract = {Despite the rich and growing body of research addressing how turnout and party choice depend on the institutional context, far less is known about the impact of the political environment on voters'propensity to vote for candidates -- not parties. Recent single-country studies have focused almost exclusively on individuallevel resource- and identity-based differences in preference voting. Combining data from the Comparative Study of Electoral Systems (CSES) and Participation and Representation in Modern Democracies (PARTIREP) election studies in six countries, this article provides the first comprehensive, cross-national test of the impact of macro-contextual factors on a voter's decision to indicate a candidate preference, instead of simply casting a party list vote. It demonstrates that both the failure of preference votes to affect the allocation of seats and choice overload dissuade voters from marking a candidate name on the ballot. These contextual factors affect informed and uninformed voters differently, moreover. The findings have important implications for electoral scholars and political practitioners when designing electoral systems.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\AIASA8ZV\André and Depauw - 2017 - Too much choice, too little impact a multilevel a.pdf}
}

@article{baileyGamificationMarketResearch2015,
  title = {Gamification in {{Market Research}}: {{Increasing Enjoyment}}, {{Participant Engagement}} and {{Richness}} of {{Data}}, but What of {{Data Validity}}?},
  shorttitle = {Gamification in {{Market Research}}},
  author = {Bailey, Pippa and Pritchard, Gareth and Kernohan, Hollie},
  year = {2015},
  month = jan,
  journal = {International Journal of Market Research},
  volume = {57},
  number = {1},
  pages = {17--28},
  issn = {1470-7853, 2515-2173},
  doi = {10.2501/IJMR-2015-003},
  urldate = {2022-01-14},
  abstract = {Research undertaken into the role of gamification in online surveys has already clearly demonstrated that applying some gamification principles can significantly increase the richness of spontaneous data and participant engagement, as well as the time that participants take to complete a survey. It is obviously appreciated that consumer engagement is critical for ensuring completion rates, reducing boredom within survey and also for panel membership moving forward, but the primary consideration and focus when designing any research survey has to be on accessing reality for the consumer and hence data validity. This paper shares the results of a research-on-research study that was conducted to understand the role of gamification, not only in terms of participant engagement and richness of data but also data validity.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\HEFTPMCF\Bailey et al. - 2015 - Gamification in Market Research Increasing Enjoym.pdf}
}

@article{bassettiCivicbaseOpensourcePlatform2023,
  title = {Civicbase: {{An}} Open-Source Platform for Deploying {{Quadratic Voting}} for {{Survey Research}}},
  shorttitle = {Civicbase},
  author = {Bassetti, Madeline E. and Dias, Gustavo and Chen, Daniel L. and Mortoni, Alan and Das, Ritesh},
  year = {2023},
  journal = {AI Magazine},
  volume = {44},
  number = {3},
  pages = {263--273},
  issn = {2371-9621},
  doi = {10.1002/aaai.12103},
  urldate = {2023-12-14},
  abstract = {Civic engagement is increasingly becoming digital. The ubiquity of computing increases our technologically mediated interactions. Governments have instated various digitization efforts to harness these new facets of virtual life. What remains to be seen is if citizen political opinion, which can inform the inception and effectiveness of public policy, is being accurately captured. Civicbase is an open-source online platform that supports the application of Quadratic Voting Survey for Research (QVSR), a novel survey method. In this paper, we explore QVSR as an effective method for eliciting policy preferences, optimal survey design for prediction, Civicbase's functionalities and technology stack, and Personal AI, an emerging domain, and its relevance to modeling individual political preferences.},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\QVPXVVU8\\Bassetti et al. - 2023 - Civicbase An open-source platform for deploying Q.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\5BRC99PZ\\aaai.html}
}

@article{benedekMeasuringDesirabilityNew,
  title = {Measuring {{Desirability}}: {{New}} Methods for Evaluating Desirability in a Usability Lab Setting},
  author = {Benedek, Joey and Miner, Trish}
}

@article{besedesReducingChoiceOverload2015,
  title = {Reducing {{Choice Overload}} without {{Reducing Choices}}},
  author = {Besede{\v s}, Tibor and Deck, Cary and Sarangi, Sudipta and Shor, Mikhael},
  year = {2015},
  month = oct,
  journal = {Review of Economics and Statistics},
  volume = {97},
  number = {4},
  pages = {793--802},
  issn = {0034-6535, 1530-9142},
  doi = {10.1162/REST_a_00506},
  urldate = {2022-05-30},
  abstract = {Previous studies have demonstrated that a multitude of options can lead to choice overload, reducing decision quality. Through controlled experiments, we examine sequential choice architectures that enable the choice set to remain large while potentially reducing the effect of choice overload. A specific tournament-style architecture achieves this goal. An alternate architecture in which subjects compare each subset of options to the most preferred option encountered thus far fails to improve performance due to the status quo bias. Subject preferences over different choice architectures are negatively correlated with performance, suggesting that providing choice over architectures might reduce the quality of decisions.},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\6KACUAKP\\rest_a_00506-esupp.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\MNT9GXU4\\Besedeš et al. - 2015 - Reducing Choice Overload without Reducing Choices.pdf}
}

@article{blasiusComparingRankingTechniques2012,
  title = {Comparing {{Ranking Techniques}} in {{Web Surveys}}},
  author = {Blasius, J{\"o}rg},
  year = {2012},
  month = nov,
  journal = {Field Methods},
  volume = {24},
  number = {4},
  pages = {382--398},
  issn = {1525-822X, 1552-3969},
  doi = {10.1177/1525822X12443095},
  urldate = {2022-01-14},
  abstract = {In an experimental split ballot design, I test four different ranking techniques (drag and drop, numbering, arrows, and most--least) to explore potential effects on substantive answers, dropouts, and item nonresponse and response time between the groups. As an example, I use six items from Inglehart's materialism--postmaterialism index. Data come from 1,225 members of an access panel who entered the set of items to be rank ordered. With respect to sex, education, and age, there are no significant differences between the four experimental groups. However, the groups differ extensively in response time, item nonresponse, and estimation of the percentage of materialists and postmaterialists. Drag and drop is shown to be the best-suited method for collecting rank data in web surveys.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\XAG736M4\Blasius - 2012 - Comparing Ranking Techniques in Web Surveys.pdf}
}

@inproceedings{bollenUnderstandingChoiceOverload2010,
  title = {Understanding Choice Overload in Recommender Systems},
  booktitle = {Proceedings of the Fourth {{ACM}} Conference on {{Recommender}} Systems - {{RecSys}} '10},
  author = {Bollen, Dirk and Knijnenburg, Bart P. and Willemsen, Martijn C. and Graus, Mark},
  year = {2010},
  pages = {63},
  publisher = {ACM Press},
  address = {Barcelona, Spain},
  doi = {10.1145/1864708.1864724},
  urldate = {2022-05-30},
  abstract = {Even though people are attracted by large, high quality recommendation sets, psychological research on choice overload shows that choosing an item from recommendation sets containing many attractive items can be a very difficult task. A web-based user experiment using a matrix factorization algorithm applied to the MovieLens dataset was used to investigate the effect of recommendation set size (5 or 20 items) and set quality (low or high) on perceived variety, recommendation set attractiveness, choice difficulty and satisfaction with the chosen item. The results show that larger sets containing only good items do not necessarily result in higher choice satisfaction compared to smaller sets, as the increased recommendation set attractiveness is counteracted by the increased difficulty of choosing from these sets. These findings were supported by behavioral measurements revealing intensified information search and increased acquisition times for these large attractive sets. Important implications of these findings for the design of recommender system user interfaces will be discussed.},
  isbn = {978-1-60558-906-0},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\9LFVX9BL\Bollen et al. - 2010 - Understanding choice overload in recommender syste.pdf}
}

@article{bramsApprovalVoting1978,
  title = {Approval {{Voting}}},
  author = {Brams, Steven J. and Fishburn, Peter C.},
  year = {1978},
  month = sep,
  journal = {American Political Science Review},
  volume = {72},
  number = {3},
  pages = {831--847},
  issn = {0003-0554, 1537-5943},
  doi = {10.2307/1955105},
  urldate = {2024-06-13},
  abstract = {Approval voting is a method of voting in which voters can vote for (``approve of'') as many candidates as they wish in an election. This article analyzes properties of this method and compares it with other single-ballot nonranked voting systems. Among the theorems proved is that approval voting is the most sincere and most strategyproof of all such voting systems; in addition, it is the only system that ensures the choice of a Condorcet majority candidate if the preferences of voters are dichotomous. Its probable empirical effects would be to (1) increase voter turnout, (2) increase the likelihood of a majority winner in plurality contests and thereby both obviate the need for runoff elections and reinforce the legitimacy of first-ballot outcomes, and (3) help centrist candidates, without at the same time denying voters the opportunity to express their support for more extremist candidates. The latter effect's institutional impact may be to weaken the two-party system yet preserve middle-of-the-road public policies of which most voters approve.},
  langid = {english}
}

@article{bramsScienceElections2001,
  title = {The {{Science}} of {{Elections}}},
  author = {Brams, Steven J. and Herschbach, Dudley R.},
  year = {2001},
  month = may,
  journal = {Science},
  volume = {292},
  number = {5521},
  pages = {1449--1449},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.292.5521.1449},
  urldate = {2024-06-13}
}

@inproceedings{budhwarPredictingVoteUsing2018,
  title = {Predicting the Vote Using Legislative Speech},
  booktitle = {Proceedings of the 19th {{Annual International Conference}} on {{Digital Government Research}}: {{Governance}} in the {{Data Age}}},
  author = {Budhwar, Aditya and Kuboi, Toshihiro and Dekhtyar, Alex and Khosmood, Foaad},
  year = {2018},
  month = may,
  series = {Dg.o '18},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3209281.3209374},
  urldate = {2022-01-14},
  abstract = {As most dedicated observers of voting bodies like the U.S. Supreme Court can attest, it is possible to guess vote outcomes based on statements made during deliberations or questioning by the voting members. We show this is also possible to do automatically using machine learning, potentially providing a powerful tool to ordinary citizens. Our working hypothesis is that verbal utterances made during the legislative process by elected representatives can indicate their intent on a future vote, and therefore can be used to automatically predict said vote to a significant degree. In this paper, we examine thousands of hours of legislative deliberations from the California state legislature's 2015-2016 session to form models of voting behavior for each legislator and use them to train classifiers and predict the votes that occur subsequent to the discussions. We can achieve average legislator vote prediction accuracies as high as 83\%. For bill vote prediction, our model can achieve 76\% accuracy with an F1 score of 0.83 using a balanced dataset.},
  isbn = {978-1-4503-6526-0},
  keywords = {digital democracy,machine learning,predictive analytics,sentiment analysis,vote prediction},
  file = {C:\Users\tcheng\Zotero\storage\6UKZ2MBQ\Budhwar et al. - 2018 - predicting the vote using legislative speech.pdf}
}

@inproceedings{byrneUsabilityVotingSystems2007,
  title = {Usability of Voting Systems: Baseline Data for Paper, Punch Cards, and Lever Machines},
  shorttitle = {Usability of Voting Systems},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Byrne, Michael D. and Greene, Kristen K. and Everett, Sarah P.},
  year = {2007},
  month = apr,
  series = {{{CHI}} '07},
  pages = {171--180},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1240624.1240653},
  urldate = {2022-12-03},
  abstract = {In the United States, computer-based voting machines are rapidly replacing other older technologies. While there is potential for this to be a usability improvement, particularly in terms of accessibility, the only way it is possible to know if usability has improved is to have baseline data on the usability of traditional technologies. We report an experiment assessing the usability of punch cards, lever machines, and two forms of paper ballot. There were no differences in ballot completion time between the four methods, but there were substantial effects on error rate, with the paper ballots superior to the other methods as well as an interaction with age of voters. Subjective usability was assessed with the System Usability Scale and showed a slight advantage for bubble-style paper ballots. Overall, paper ballots were found to be particularly usable, which raises important technological and policy issues.},
  isbn = {978-1-59593-593-9},
  keywords = {DRE,effectiveness,efficiency,subjective usability,voting},
  file = {C:\Users\tcheng\Zotero\storage\UCMB2SES\Byrne et al. - 2007 - Usability of voting systems baseline data for pap.pdf}
}

@article{carmonaPublicisationPrivateSpace2022,
  title = {The ``Public-Isation'' of Private Space -- towards a Charter of Public Space Rights and Responsibilities},
  author = {Carmona, Matthew},
  year = {2022},
  month = apr,
  journal = {Journal of Urbanism: International Research on Placemaking and Urban Sustainability},
  volume = {15},
  number = {2},
  pages = {133--164},
  publisher = {Routledge},
  issn = {1754-9175},
  doi = {10.1080/17549175.2021.1887324},
  urldate = {2023-08-29},
  abstract = {There has been much written about the ``privatisation of public space''. This paper explores and challenges these narratives by questioning whether we have seen a privatisation at all. Through an analysis of historic and contemporary data, it concludes that, in London at least, we have actually witnessed the reverse, a ``public-isation of private space''. The paper goes on to ask what are the management implications of the trend? It finds that the negative associations around privatisation are often misplaced and that public-isation processes have the potential to deliver a substantial net gain to society. At the same time, the public interest management implications are just as real for public-isation as for privatisation processes. Through action research the idea of public authorities adopting a charter of public space rights and responsibilities is tested in order that the potential benefits of public space projects are captured and negative impacts avoided.},
  keywords = {charter,privatisation,Public space,public-isation,publicness},
  file = {C:\Users\tcheng\Zotero\storage\KWR5Z72F\Carmona - 2022 - The “public-isation” of private space – towards a .pdf}
}

@article{cavailleWhoCaresMeasuring,
  title = {Who {{Cares}}? {{Measuring Preference Intensity}} in a {{Polarized Environment}}},
  author = {Cavaille, Charlotte and Chen, Daniel L},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\ANDAME6J\Cavaille and Chen - Who Cares Measuring Preference Intensity in a Pol.pdf}
}

@article{chengCanShowWhat2021,
  title = {"{{I}} Can Show What {{I}} Really like.": {{Eliciting Preferences}} via {{Quadratic Voting}}},
  shorttitle = {"{{I}} Can Show What {{I}} Really like."},
  author = {Cheng, Ti-Chung and Li, Tiffany and Chou, Yi-Hung and Karahalios, Karrie and Sundaram, Hari},
  year = {2021},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  pages = {1--43},
  doi = {10.1145/3449281},
  abstract = {Surveys are a common instrument to gauge self-reported opinions from the crowd for scholars in the CSCW community, the social sciences, and many other research areas. Researchers often use surveys to prioritize a subset of given options when there are resource constraints. Over the past century, researchers have developed a wide range of surveying techniques, including one of the most popular instruments, the Likert ordinal scale [49], to elicit individual preferences. However, the challenge to elicit accurate and rich self-reported responses with surveys in a resource-constrained context still persists today. In this study, we examine Quadratic Voting (QV), a voting mechanism powered by the affordances of a modern computer and straddles ratings and rankings approaches [64], as an alternative online survey technique.We argue that QV could elicit more accurate self-reported responses compared to the Likert scale when the goal is to understand relative preferences under resource constraints. We conducted two randomized controlled experiments on Amazon Mechanical Turk, one in the context of public opinion polling and the other in a human-computer interaction user study. Based on our Bayesian analysis results, a QV survey with a sufficient amount of voice credits, aligned significantly closer to participants' incentive-compatible behaviors than a Likert scale survey, with a medium to high effect size. In addition, we extended QV's application scenario from typical public policy and education research to a problem setting familiar to the CSCW community: a prototypical HCI user study. Our experiment results, QV survey design, and QV interface serve as a stepping stone for CSCW researchers to further explore this surveying methodology in their studies and encourage decision-makers from other communities to consider QV as a promising alternative.},
  file = {C:\Users\tcheng\Zotero\storage\XDAKB8NE\Cheng et al. - 2021 - I can show what I really like. Eliciting Prefer.pdf}
}

@article{chernevChoiceOverloadConceptual2015,
  title = {Choice Overload: {{A}} Conceptual Review and Meta-Analysis},
  shorttitle = {Choice Overload},
  author = {Chernev, Alexander and B{\"o}ckenholt, Ulf and Goodman, Joseph},
  year = {2015},
  month = apr,
  journal = {Journal of Consumer Psychology},
  volume = {25},
  number = {2},
  pages = {333--358},
  issn = {10577408},
  doi = {10.1016/j.jcps.2014.08.002},
  urldate = {2022-07-29},
  abstract = {Despite the voluminous evidence in support of the paradoxical finding that providing individuals with more options can be detrimental to choice, the question of whether and when large assortments impede choice remains open. Even though extant research has identified a variety of antecedents and consequences of choice overload, the findings of the individual studies fail to come together into a cohesive understanding of when large assortments can benefit choice and when they can be detrimental to choice. In a meta-analysis of 99 observations (N = 7202) reported by prior research, we identify four key factors---choice set complexity, decision task difficulty, preference uncertainty, and decision goal---that moderate the impact of assortment size on choice overload. We further show that each of these four factors has a reliable and significant impact on choice overload, whereby higher levels of decision task difficulty, greater choice set complexity, higher preference uncertainty, and a more prominent, effort-minimizing goal facilitate choice overload. We also find that four of the measures of choice overload used in prior research---satisfaction/confidence, regret, choice deferral, and switching likelihood---are equally powerful measures of choice overload and can be used interchangeably. Finally, we document that when moderating variables are taken into account the overall effect of assortment size on choice overload is significant---a finding counter to the data reported by prior meta-analytic research.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\LBT5ZQJV\Chernev et al. - 2015 - Choice overload A conceptual review and meta-anal.pdf}
}

@article{cmPROCEEDINGSFIFTHCONFERENCE,
  title = {{{PROCEEDINGS OF THE FIFTH CONFERENCE ON THE DESIGN OF EXPERIMENTS IN ARMY RESEARCH DEVELOPMENT AUD TESTING}}},
  author = {Cm, Box and Carolina, North},
  pages = {384},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\QRF8PZE7\Cm and Carolina - PROCEEDINGS OF THE FIFTH CONFERENCE ON THE DESIGN .pdf}
}

@article{conradElectronicVotingEliminates2009,
  title = {Electronic Voting Eliminates Hanging Chads but Introduces New Usability Challenges},
  author = {Conrad, Frederick G. and Bederson, Benjamin B. and Lewis, Brian and Peytcheva, Emilia and Traugott, Michael W. and Hanmer, Michael J. and Herrnson, Paul S. and Niemi, Richard G.},
  year = {2009},
  month = jan,
  journal = {International Journal of Human-Computer Studies},
  volume = {67},
  number = {1},
  pages = {111--124},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2008.09.010},
  urldate = {2023-12-16},
  abstract = {The arrival of electronic voting has generated considerable controversy, mostly about its vulnerability to fraud. By comparison, virtually no attention has been given to its usability, i.e., voters' ability to vote as they intend, which was central to the controversy surrounding the 2000 US presidential election. Yet it is hard to imagine a domain of human--computer interaction where usability has more impact on how democracy works. This article reports a laboratory investigation of the usability of six electronic voting systems chosen to represent the features of systems in current use and potentially in future use. The primary question was whether e-voting systems are sufficiently hard to use that voting accuracy and satisfaction are compromised. We observed that voters often seemed quite lost taking far more than the required number of actions to cast individual votes, especially when they ultimately voted inaccurately. Their satisfaction went down as their effort went up. And accuracy with some systems was disturbingly low. While many of these problems are easy to fix, manufacturers will need to adopt usability engineering practices that have vastly improved user interfaces throughout the software industry.},
  keywords = {Usability of e-voting,Usability of electronic voting,Voting interfaces}
}

@article{couperWebSurveyDesign2001,
  title = {Web Survey Design and Administration},
  author = {Couper, M. P.},
  year = {2001},
  journal = {Public Opinion Quarterly},
  volume = {65},
  number = {2},
  pages = {230--253},
  issn = {0033-362X},
  doi = {10.1086/322199},
  abstract = {Many claims are being made about the advantages of conducting surveys on the Web. However, there has been little research on the effects of format or design on the levels of unit and item response or on data quality. In a study conducted at the University of Michigan, a number of experiments were added to a survey of the student population to assess the impact of design features on resulting data quality. A sample of 1,602 students was sent an e-mail invitation to participate in a Web survey on attitudes toward affirmative action. Three experiments on design approaches were added to the survey application. One experiment varied whether respondents were reminded of their progress through the instrument. In a second experiment, one version presented several related items on one screen, while the other version presented one question per screen. In a third experiment, for one series of questions a random half of the sample clicked radio buttons to indicate their answers, while the other half entered a numeric response in a box. This article discusses the overall implementation and outcome of the survey, and it describes the results of the imbedded design experiments.},
  langid = {english},
  pmid = {11420757}
}

@article{couperWhatTheySee2004,
  title = {What {{They See Is What We Get}}: {{Response Options}} for {{Web Surveys}}},
  shorttitle = {What {{They See Is What We Get}}},
  author = {Couper, Mick P. and Tourangeau, Roger and Conrad, Frederick G. and Crawford, Scott D.},
  year = {2004},
  month = feb,
  journal = {Social Science Computer Review},
  volume = {22},
  number = {1},
  pages = {111--127},
  publisher = {SAGE Publications Inc},
  issn = {0894-4393},
  doi = {10.1177/0894439303256555},
  urldate = {2022-08-02},
  abstract = {Several alternative response formats are available to the web survey designer, but the choice of format is often made with little consideration of measurement error. The authors experimentally explore three common response formats used in web surveys: a series of radio buttons, a drop box with none of the options initially displayed until the respondent clicks on the box, and a scrollable drop box with some of the options initially visible, requiring the respondent to scroll to see the remainder of the options. The authors reversed the order of the response options for half the sample. The authors find evidence of response order effects but stronger evidence that visible response options are endorsed more frequently, suggesting that visibility may be a more powerful effect than primacy in web surveys. The results suggest that the response format used in web surveys does affect the choices made by respondents.},
  file = {C:\Users\tcheng\Zotero\storage\VNTDJEVD\Couper et al. - 2004 - What They See Is What We Get Response Options for.pdf}
}

@article{cunowLessMoreParadox2021,
  title = {Less Is More: {{The}} Paradox of Choice in Voting Behavior},
  shorttitle = {Less Is More},
  author = {Cunow, Saul and Desposato, Scott and Janusz, Andrew and Sells, Cameron},
  year = {2021},
  month = feb,
  journal = {Electoral Studies},
  volume = {69},
  pages = {102230},
  issn = {02613794},
  doi = {10.1016/j.electstud.2020.102230},
  urldate = {2022-05-30},
  abstract = {How does the number of candidates competing in an election affect voting behavior? In theory, as the number of candidates running for office increase, citizens' utility from voting also increases. With more candidates, voters are more likely to have candidates that are close to their ideal points. Practically, however, more candidates also means a higher cognitive burden for voters who must learn more during campaigns in order to find their ``ideal'' candidate. In this paper, we examine how choice set size affects voting behavior. Using a survey experiment, we show that subjects presented with many options learn less about candidates, are more likely to vote based on meaningless heuristics, and are more likely to commit voting errors, when compared with subjects who choose between only a few candidates.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\WTEDWWYX\Cunow et al. - 2021 - Less is more The paradox of choice in voting beha.pdf}
}

@book{daniel2017thinking,
  title = {Thinking, Fast and Slow},
  author = {Daniel, Kahneman},
  year = {2017}
}

@inproceedings{dawkinsPrimeIIIInnovative2009,
  title = {Prime {{III}}: An Innovative Electronic Voting Interface},
  shorttitle = {Prime {{III}}},
  booktitle = {Proceedings of the 14th International Conference on {{Intelligent}} User Interfaces},
  author = {Dawkins, Shane{\'e} and Sullivan, Tony and Rogers, Greg and Cross, E. Vincent and Hamilton, Lauren and Gilbert, Juan E.},
  year = {2009},
  month = feb,
  series = {{{IUI}} '09},
  pages = {485--486},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1502650.1502727},
  urldate = {2023-04-27},
  abstract = {Voting technology today has not addressed the issues that disabled voters are confronted with at the polls. Because approximately 17\% of the voting population is disabled, their issues should be handled with a solution geared towards their needs. Disabled voters need to be able to cast their vote without the assistance of others. The Prime III multimodal voting system [2] addresses these issues. This demonstration will illustrate the use of the Prime III system, a virtual reality (VR) version (Prime V), and a similar version created using a voice user interface (VUI).},
  isbn = {978-1-60558-168-2},
  keywords = {e-voting,multimodal user interaction,universal access},
  file = {C:\Users\tcheng\Zotero\storage\QDM2DYUN\Dawkins et al. - 2009 - Prime III an innovative electronic voting interfa.pdf}
}

@article{deciEffectsContingentNoncontingent1972,
  title = {The {{Effects}} of {{Contingent}} and {{Noncontingent Reward}} and {{Controls}} on {{Intrinsic Motivation}}},
  author = {Deci, Edward},
  year = {1972},
  month = oct,
  journal = {Organizational Behavior and Human Performance},
  volume = {8},
  pages = {217--229},
  doi = {10.1016/0030-5073(72)90047-5},
  abstract = {Theories of management and work motivation distinguish between two kinds of rewards---extrinsic and intrinsic. Extrinsic rewards are ones such as money and verbal reinforcement which are mediated outside of the person, whereas intrinsic rewards are mediated within the person. We say a person is intrinsically motivated to perform an activity if there is no apparent reward except the activity itself or the feelings which result from the activity. All of the theories of work motivation which consider both kinds of rewards assume that the effects of the two are additive. This paper examines that assumption by reviewing a program of research which investigated the effects of external rewards and controls on intrinsic motivation. It was reported that a person's intrinsic motivation to perform an activity decreased when he received contingent monetary payments, threats of punishment for poor performance, or negative feedback about his performance. Noncontingent monetary payments left intrinsic motivation unchanged, and verbal reinforcements appeared to enhance intrinsic motivation. A cognitive evaluation theory was presented to explain these results, and the theory and results were discussed in relation to management.}
}

@article{deckEffectCognitiveLoad2015,
  title = {The Effect of Cognitive Load on Economic Decision Making: {{A}} Survey and New Experiments},
  shorttitle = {The Effect of Cognitive Load on Economic Decision Making},
  author = {Deck, Cary and Jahedi, Salar},
  year = {2015},
  month = aug,
  journal = {European Economic Review},
  volume = {78},
  pages = {97--119},
  issn = {00142921},
  doi = {10.1016/j.euroecorev.2015.05.004},
  urldate = {2022-06-10},
  abstract = {Psychologists and economists have examined the effect of cognitive load in a variety of situations from risk taking to snack choice. We review previous experiments that have directly manipulated cognitive load and summarize their findings. We report the results of two new experiments where participants engage in a digit-memorization task while simultaneously performing a variety of economic tasks including: (1) choices involving risk, (2) choices involving intertemporal substitution, (3) choices with anchoring effects, (4) choices over healthy and unhealthy snacks, and (5) math problems. We find that higher cognitive load reduces numeracy as measured by performance in math problems. Moreover, within-subject analysis indicates that cognitive load leads to more risk-averse behavior, more impatience over money, and (nominally) more likelihood to anchor. We do not find any evidence that cognitive load increases impatience over consumption goods or unhealthy snack choices. Exploiting the panel nature of our data set, we find that those individuals who are most sensitive to cognitive load, as measured by a large drop in their own math performance across 1- and 8-digit memorization treatments, are driving much of the effect.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\YFFS4II9\Deck and Jahedi - 2015 - The effect of cognitive load on economic decision .pdf}
}

@article{demariaPeoplePayReduce,
  title = {Do People Pay to Reduce Their Decision Space When Faced with Too Many Options? {{An}} Experimental Approach.},
  author = {Demar{\'i}a, Sydney},
  pages = {42},
  abstract = {Standard economic theory posits that having more choice is always better than having less. However, previous research shows that having too many options may lead to choice overload, lowering decision quality and even impairing it altogether. Through a set of economic experiments I examine whether subjects are willing to pay to reduce their decision space if faced with too many options. Three decision curtailing conditions are considered: i) reducing the choice set, ii) delegating the choice, iii) not choosing. Participants (N = 133) were randomly assigned to two treatments: i) large choice set, ii) small choice set. In each study subjects could pick between a series of abstract lotteries and a stochastically dominated choice curtailing option. The relative frequencies of this latter alternative is the core interest of the studies. I find a statistically significant effect of the size of the choice set on the decision to pay to forego decision rights, by randomizing the decision, (N = 44, p = 0,045). Likewise, the size of the choice set significantly affects the decision to pay to opt out of the task (N = 44, p = 0,031). On the contrary, the size of the set exerted no significant effect on the decision to pay to reduce it (N = 45, p = 0,143). This research contributes to the literature on decision-making by relating a market mechanism such as prices to counter choice overload. Moreover, these findings suggest and increased role for pricing strategies and product design, especially considering the increasing accuracy of machine learning prediction algorithms.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\VSA8N56Q\Demaría - Do people pay to reduce their decision space when .pdf}
}

@inproceedings{disciascioRankYouGo2016,
  title = {Rank {{As You Go}}: {{User-Driven Exploration}} of {{Search Results}}},
  shorttitle = {Rank {{As You Go}}},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {{di Sciascio}, Cecilia and Sabol, Vedran and Veas, Eduardo E.},
  year = {2016},
  month = mar,
  pages = {118--129},
  publisher = {ACM},
  address = {Sonoma California USA},
  doi = {10.1145/2856767.2856797},
  urldate = {2022-05-31},
  abstract = {Whenever users engage in gathering and organizing new information, searching and browsing activities emerge at the core of the exploration process. As the process unfolds and new knowledge is acquired, interest drifts occur inevitably and need to be accounted for. Despite the advances in retrieval and recommender algorithms, real-world interfaces have remained largely unchanged: results are delivered in a relevance-ranked list. However, it quickly becomes cumbersome to reorganize resources along new interests, as any new search brings new results. We introduce uRank and investigate interactive methods for understanding, refining and reorganizing documents on-the-fly as information needs evolve. uRank includes views summarizing the contents of a recommendation set and interactive methods conveying the role of users' interests through a recommendation ranking. A formal evaluation showed that gathering items relevant to a particular topic of interest with uRank incurs in lower cognitive load compared to a traditional ranked list. A second study consisting in an ecological validation reports on usage patterns and usability of the various interaction techniques within a free, more natural setting.},
  isbn = {978-1-4503-4137-0},
  langid = {english},
  keywords = {tagging},
  file = {C:\Users\tcheng\Zotero\storage\59XMCT8J\di Sciascio et al. - 2016 - Rank As You Go User-Driven Exploration of Search .pdf}
}

@article{duffyRELATIVERATINGSYSTEMS1974,
  title = {{{ON}} "{{RELATIVE}}" {{RATING SYSTEMS}}},
  author = {Duffy, Kirt E. and Webber, Robert E.},
  year = {1974},
  month = jun,
  journal = {Personnel Psychology},
  volume = {27},
  number = {2},
  pages = {307--311},
  issn = {0031-5826, 1744-6570},
  doi = {10.1111/j.1744-6570.1974.tb01536.x},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\2ESMNM23\Duffy and Webber - 1974 - ON RELATIVE RATING SYSTEMS.pdf}
}

@misc{ElectronicVotingSystem,
  title = {Electronic Voting System Usability Issues {\textbar} {{Proceedings}} of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/642611.642638},
  file = {C:\Users\tcheng\Zotero\storage\SUNZ9TLS\642611.html}
}

@book{engstrom2020politics,
  title = {The Politics of Ballot Design: How States Shape {{American}} Democracy},
  author = {Engstrom, Erik J and Roberts, Jason M},
  year = {2020},
  publisher = {Cambridge University Press}
}

@inproceedings{everettElectronicVotingMachines2008,
  title = {Electronic Voting Machines versus Traditional Methods: Improved Preference, Similar Performance},
  shorttitle = {Electronic Voting Machines versus Traditional Methods},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Everett, Sarah P. and Greene, Kristen K. and Byrne, Michael D. and Wallach, Dan S. and Derr, Kyle and Sandler, Daniel and Torous, Ted},
  year = {2008},
  month = apr,
  series = {{{CHI}} '08},
  pages = {883--892},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1357054.1357195},
  urldate = {2022-10-13},
  abstract = {In the 2006 U.S. election, it was estimated that over 66 million people would be voting on direct recording electronic (DRE) systems in 34\% of the nation's counties [8]. Although these computer-based voting systems have been widely adopted, they have not been empirically proven to be more usable than their predecessors. The series of studies reported here compares usability data from a DRE with those from more traditional voting technologies (paper ballots, punch cards, and lever machines). Results indicate that there were little differences between the DRE and these older methods in efficiency or effectiveness. However, in terms of user satisfaction, the DRE was significantly better than the older methods. Paper ballots also perform well, but participants were much more satisfied with their experiences voting on the DRE. The disconnect between subjective and objective usability has potential policy ramifications.},
  isbn = {978-1-60558-011-1},
  keywords = {dre,electronic voting,preference,usability,voting},
  file = {C:\Users\tcheng\Zotero\storage\74AKM33C\Everett et al. - 2008 - Electronic voting machines versus traditional meth.pdf}
}

@misc{ExperiencesElectronicVote,
  title = {Experiences with {{Electronic Vote}} {\textbar} {{Proceedings}} of the 9th {{International Conference}} on {{Theory}} and {{Practice}} of {{Electronic Governance}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/2910019.2910098},
  file = {C:\Users\tcheng\Zotero\storage\RU8NP54F\2910019.html}
}

@misc{experienceUsingMicrosoftDesirability,
  title = {Using the {{Microsoft Desirability Toolkit}} to {{Test Visual Appeal}}},
  author = {Experience, World Leaders in Research-Based User},
  journal = {Nielsen Norman Group},
  urldate = {2022-01-14},
  abstract = {Measure people's attitudes toward a user interface by a controlled vocabulary test: give users the list of product reaction words and asks them to select those that best describe the design.},
  howpublished = {https://www.nngroup.com/articles/microsoft-desirability-toolkit/},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\LU55YQ54\microsoft-desirability-toolkit.html}
}

@inproceedings{farzandAestheticsEvaluatingResponse2024,
  title = {Beyond {{Aesthetics}}: {{Evaluating Response Widgets}} for {{Reliability}} \& {{Construct Validity}} of {{Scale Questionnaires}}},
  shorttitle = {Beyond {{Aesthetics}}},
  booktitle = {Extended {{Abstracts}} of the 2024 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Farzand, Habiba and Al Baiaty Suarez, David and Goodge, Thomas and Macdonald, Shaun Alexander and Marky, Karola and Khamis, Mohamed and Cairns, Paul},
  year = {2024},
  month = may,
  series = {{{CHI EA}} '24},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613905.3650751},
  urldate = {2024-06-13},
  abstract = {Scale questionnaires are psychometric tools that capture perspectives and experiences. Consequently, these tools need to be reliable and valid. In this paper, we investigate the impact of response widgets - the UI elements that allow users to answer scale items - on the overall scale reliability and construct validity of three varied length scale questionnaires in a user study (N=30). Our results reveal that optimum reliability was achieved using radio buttons and dropdowns in all varied-length questionnaires. Further, valid results were produced utilising the slider and dropdown. No significant differences were found in time consumption, but click count was significantly higher with dropdown. Radio buttons scored lower in format satisfaction than others, and dropdown was the least effective in ease of selection and quick completion. In light of these results, we conclude that response widgets are more than just aesthetics and should be selected as per the researcher's aims.},
  isbn = {9798400703317},
  keywords = {reliability,scale questionnaires,UI response styles,user experience,validity},
  file = {C:\Users\tcheng\Zotero\storage\QENC33TZ\Farzand et al. - 2024 - Beyond Aesthetics Evaluating Response Widgets for.pdf}
}

@article{featherMeasurementValuesEffects1973,
  title = {The Measurement of Values: {{Effects}} of Different Assessment Procedures},
  shorttitle = {The Measurement of Values},
  author = {Feather, N. T.},
  year = {1973},
  month = dec,
  journal = {Australian Journal of Psychology},
  volume = {25},
  number = {3},
  pages = {221--231},
  issn = {0004-9530, 1742-9536},
  doi = {10.1080/00049537308255849},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\7MTFFIXE\Feather - 1973 - The measurement of values Effects of different as.pdf}
}

@inproceedings{fr?kj?rMeasuringUsabilityAre2000,
  title = {Measuring Usability: Are Effectiveness, Efficiency, and Satisfaction Really Correlated?},
  shorttitle = {Measuring Usability},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems  - {{CHI}} '00},
  author = {Fr?kj?r, Erik and Hertzum, Morten and Hornb{\ae}k, Kasper},
  year = {2000},
  pages = {345--352},
  publisher = {ACM Press},
  address = {The Hague, The Netherlands},
  doi = {10.1145/332040.332455},
  urldate = {2022-05-31},
  abstract = {Usability comprises the aspects effectiveness, efficiency, and satisfaction. The correlations between these aspects are not well understood for complex tasks. We present data from an experiment where 87 subjects solved 20 information retrieval tasks concerning programming problems. The correlation between efficiency, as indicated by task completion time, and effectiveness, as indicated by quality of solution, was negligible. Generally, the correlations among the usability aspects depend in a complex way on the application domain, the user's experience, and the use context. Going through three years of CHI Proceedings, we find that 11 out of 19 experimental studies involving complex tasks account for only one or two aspects of usability. When these studies make claims concerning overall usability, they rely on risky assumptions about correlations between usability aspects. Unless domain specific studies suggest otherwise, effectiveness, efficiency, and satisfaction should be considered independent aspect of usability and all be included in usability testing.},
  isbn = {978-1-58113-216-8},
  langid = {english},
  keywords = {effectiveness,efficiency,information retrival,satisfaction,usability measures,usability testing,user studies},
  file = {C:\Users\tcheng\Zotero\storage\LTKQ62WZ\Frkjr et al. - 2000 - Measuring usability are effectiveness, efficiency.pdf}
}

@inproceedings{fridmanCognitiveLoadEstimation2018,
  title = {Cognitive {{Load Estimation}} in the {{Wild}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fridman, Lex and Reimer, Bryan and Mehler, Bruce and Freeman, William T.},
  year = {2018},
  month = apr,
  pages = {1--9},
  publisher = {ACM},
  address = {Montreal QC Canada},
  doi = {10.1145/3173574.3174226},
  urldate = {2022-05-31},
  abstract = {Cognitive load has been shown, over hundreds of validated studies, to be an important variable for understanding human performance. However, establishing practical, non-contact approaches for automated estimation of cognitive load under real-world conditions is far from a solved problem. Toward the goal of designing such a system, we propose two novel vision-based methods for cognitive load estimation, and evaluate them on a large-scale dataset collected under real-world driving conditions. Cognitive load is defined by which of 3 levels of a validated reference task the observed subject was performing. On this 3-class problem, our best proposed method of using 3D convolutional neural networks achieves 86.1\% accuracy at predicting task-induced cognitive load in a sample of 92 subjects from video alone. This work uses the driving context as a training and evaluation dataset, but the trained network is not constrained to the driving environment as it requires no calibration and makes no assumptions about the subject's visual appearance, activity, head pose, scale, and perspective.},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\P88EQVCD\Fridman et al. - 2018 - Cognitive Load Estimation in the Wild.pdf}
}

@article{galyWhatRelationshipMental2012,
  title = {What Is the Relationship between Mental Workload Factors and Cognitive Load Types?},
  author = {Galy, Edith and Cariou, Magali and M{\'e}lan, Claudine},
  year = {2012},
  month = mar,
  journal = {International Journal of Psychophysiology},
  volume = {83},
  number = {3},
  pages = {269--275},
  issn = {01678760},
  doi = {10.1016/j.ijpsycho.2011.09.023},
  urldate = {2022-05-31},
  abstract = {The present study tested the hypothesis of an additive interaction between intrinsic, extraneous and germane cognitive load, by manipulating factors of mental workload assumed to have a specific effect on either type of cognitive load. The study of cognitive load factors and their interaction is essential if we are to improve workers' wellbeing and safety at work. High cognitive load requires the individual to allocate extra resources to entering information. It is thought that this demand for extra resources may reduce processing efficiency and performance. The present study tested the effects of three factors thought to act on either cognitive load type, i.e. task difficulty, time pressure and alertness in a working memory task. Results revealed additive effects of task difficulty and time pressure, and a modulation by alertness on behavioral, subjective and psychophysiological workload measures. Mental overload can be the result of a combination of task-related components, but its occurrence may also depend on subject-related characteristics, including alertness. Solutions designed to reduce incidents and accidents at work should consider work organization in addition to task constraints in so far that both these factors may interfere with mental workload.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\T9Q8TNPT\Galy et al. - 2012 - What is the relationship between mental workload f.pdf}
}

@article{genterDragandDropNumericEntry2022,
  title = {Drag-and-{{Drop Versus Numeric Entry Options}}: {{A Comparison}} of {{Survey Ranking Questions}} in {{Qualtrics}}},
  author = {Genter, Shaun and Trejo, Yazm{\'i}n Garc{\'i}a and Nichols, Elizabeth},
  year = {2022},
  volume = {17},
  number = {3},
  pages = {14},
  abstract = {An online feedback survey for the 2020 United States Census Partnership program included a question asking respondents to rank the amount of time they spent on six job-related tasks. For this study, we randomly assigned respondents to answer this question using either the numeric entry or dragand-drop option available in Qualtrics™. We compared the two designs by their distribution of rankings and by the amount of time respondents spent on-screen to answer the question. The distribution of rankings did not vary by design, and we found no differences in time-on-task.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\ICHGAUSL\Genter et al. - 2022 - Drag-and-Drop Versus Numeric Entry Options A Comp.pdf}
}

@article{gerjetsScientificValueCognitive2009,
  title = {The {{Scientific Value}} of {{Cognitive Load Theory}}: {{A Research Agenda Based}} on the {{Structuralist View}} of {{Theories}}},
  shorttitle = {The {{Scientific Value}} of {{Cognitive Load Theory}}},
  author = {Gerjets, Peter and Scheiter, Katharina and Cierniak, Gabriele},
  year = {2009},
  month = mar,
  journal = {Educational Psychology Review},
  volume = {21},
  number = {1},
  pages = {43--54},
  issn = {1040-726X, 1573-336X},
  doi = {10.1007/s10648-008-9096-1},
  urldate = {2022-01-14},
  abstract = {In this paper, two methodological perspectives are used to elaborate on the value of cognitive load theory (CLT) as a scientific theory. According to the more traditional critical rationalism of Karl Popper, CLT cannot be considered a scientific theory because some of its fundamental assumptions cannot be tested empirically and are thus not falsifiable. According to the structuralist view of theories introduced by Joseph D. Sneed, a theory may be considered scientific even if it comprises nontestable fundamental assumptions. Rather, the scientific value of a theory results from the holistic empirical content of the overall theory net built around fundamental assumptions and from the successful applications of this theory net to explain and predict empirical findings. This latter view is helpful to explicate some implicit methodological assumptions of CLT research and to avoid the potential circularity of CLT's fundamental assumptions. Additionally, the structuralist view of theories can be directly used to derive a research agenda for the future development of CLT.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\C6BSYZC6\Gerjets et al. - 2009 - The Scientific Value of Cognitive Load Theory A R.pdf}
}

@article{gilbertAnomalyDetectionElectronic2013,
  title = {Anomaly Detection in Electronic Voting Systems},
  author = {Gilbert, Juan E. and Dunbar, Jerone and Ottley, Alvitta and Smotherman, John Mark},
  year = {2013},
  month = sep,
  journal = {Information Design Journal (IDJ)},
  volume = {20},
  number = {3},
  pages = {194--206},
  issn = {01425471},
  doi = {10.1075/idj.20.3.01gil},
  abstract = {Studies have shown that voting error remains a problem with Direct Recording Electronic (DRE) voting machines. DRE's have an advantage over other voting technologies by facilitating ballot verification through review screens. However, results from ballot verification studies have shown that no more than half of study participants notice review screen anomalies (Campbell \& Byrne, 2009). This research replicated previous studies on anomaly detection on review screens using a multimodal voting system called Prime III. The results suggest that Prime III facilitates ballot verification and effectively yielded a detection rate of 90\%, even without informing participants on the importance of ballot verification.},
  keywords = {Anomaly detection (Computer security),Ballots,DRE,Electronic voting,electronic voting anomalies,HCI) and User Interfaces,Prime III,Verification of computer systems,VoteBox. Information interfaces and presentation (e.g.,Voting machines}
}

@inproceedings{gilbertDesigningSocialTranslucence2012,
  title = {Designing Social Translucence over Social Networks},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gilbert, Eric},
  year = {2012},
  month = may,
  pages = {2731--2740},
  publisher = {ACM},
  address = {Austin Texas USA},
  doi = {10.1145/2207676.2208670},
  urldate = {2022-01-19},
  abstract = {Social translucence is a landmark theory in social computing. Modeled on physical life, it guides designers toward elegant social technologies. However, we argue that it breaks down over modern social network sites because social networks resist its physical metaphors. In this paper, we build theory relating social translucence to social network structure. To explore this idea, we built a tool called Link Different. Link Different addresses a structural awareness problem by letting users know how many of their Twitter followers already a saw link via someone else they follow. During two months on the web, nearly 150K people used the site a total of 1.3M times. Its widespread, viral use suggests that people want social translucence, but network structure gets in the way. We conclude the paper by illustrating new design problems that lie at the intersection of social translucence and other unexplored network structures.},
  isbn = {978-1-4503-1015-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\5XESJVWY\\Gilbert - 2012 - Designing social translucence over social networks.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\JFB7QK2I\\Gilbert - 2012 - Designing social translucence over social networks.pdf}
}

@article{goelKnapsackVotingVoting,
  title = {Knapsack {{Voting}}: {{Voting}} Mechanisms for {{Participatory Budgeting}}},
  author = {Goel, Ashish and Krishnaswamy, Anilesh K and Sakshuwong, Sukolsak and Aitamurto, Tanja},
  abstract = {We address the question of aggregating the preferences of voters in the context of participatory budgeting. We scrutinize the voting method currently used in practice, underline its drawbacks, and introduce a novel scheme tailored to this setting, which we call ``Knapsack Voting''. We study its strategic properties - we show that it is strategy-proof under a natural model of utility (a dis-utility given by the 1 distance between the outcome and the true preference of the voter), and ``partially'' strategy-proof under general additive utilities. We extend Knapsack Voting to more general settings with revenues, deficits or surpluses, and prove a similar strategy-proofness result. To further demonstrate the applicability of our scheme, we discuss its implementation on the digital voting platform that we have deployed in partnership with the local government bodies in many cities across the nation. From voting data thus collected, we present empirical evidence that Knapsack Voting works well in practice.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\WSQDYDLC\Goel et al. - Knapsack Voting Voting mechanisms for Participato.pdf}
}

@article{golderUsagePatternsCollaborative2006,
  title = {Usage Patterns of Collaborative Tagging Systems},
  author = {Golder, Scott A. and Huberman, Bernardo A.},
  year = {2006},
  month = apr,
  journal = {Journal of Information Science},
  volume = {32},
  number = {2},
  pages = {198--208},
  publisher = {SAGE Publications Ltd},
  issn = {0165-5515},
  doi = {10.1177/0165551506062337},
  urldate = {2022-07-26},
  abstract = {Collaborative tagging describes the process by which many users add metadata in the form of keywords to shared content. Recently, collaborative tagging has grown in popularity on the web, on sites that allow users to tag bookmarks, photographs and other content. In this paper we analyze the structure of collaborative tagging systems as well as their dynamic aspects. Specifically, we discovered regularities in user activity, tag frequencies, kinds of tags used, bursts of popularity in bookmarking and a remarkable stability in the relative proportions of tags within a given URL. We also present a dynamic model of collaborative tagging that predicts these stable patterns and relates them to imitation and shared knowledge.},
  file = {C:\Users\tcheng\Zotero\storage\UCDGWI99\Golder and Huberman - 2006 - Usage patterns of collaborative tagging systems.pdf}
}

@article{gourvilleOverchoiceAssortmentType2005,
  title = {Overchoice and {{Assortment Type}}: {{When}} and {{Why Variety Backfires}}},
  shorttitle = {Overchoice and {{Assortment Type}}},
  author = {Gourville, John T. and Soman, Dilip},
  year = {2005},
  month = aug,
  journal = {Marketing Science},
  volume = {24},
  number = {3},
  pages = {382--395},
  publisher = {INFORMS},
  issn = {0732-2399},
  doi = {10.1287/mksc.1040.0109},
  urldate = {2024-06-13},
  abstract = {Almost universally, research and practice suggest that a brand that increases its product assortment, or variety, should benefit through increased market share. In this paper, we show this is not always the case. We introduce the construct ``assortment type'' and demonstrate that the effect of assortment size on brand share is systematically moderated by assortment type. We define an ``alignable'' assortment as a set of brand variants that differ along a single, compensatory dimension such that choosing from that assortment only requires within-attribute trade-offs. In contrast, we define a ``nonalignable'' assortment as a set of brand variants that simultaneously vary along multiple, noncompensatory dimensions, demanding between-attribute trade-offs. In turn, we argue that an alignable assortment can efficiently meet the diverse tastes of consumers, thereby increasing brand share, but that a nonalignable assortment increases both the cognitive effort and the potential for regret faced by a consumer, thereby decreasing brand share. We term this effect ``overchoice.'' Across three studies, we provide evidence of overchoice and tie the effect to the effort and regret brought about by nonalignability. In the process, we demonstrate that simplification of information presentation, reversibility of choice, and a reduction in underlying nonalignability serve to reduce or eliminate this effect.},
  keywords = {brand assortments,brand choice,decision making,product policy,variety},
  file = {C:\Users\tcheng\Zotero\storage\S4SCJXMU\Gourville and Soman - 2005 - Overchoice and Assortment Type When and Why Varie.pdf}
}

@misc{Gov4gitDecentralizedPlatform2023,
  title = {Gov4git: {{A Decentralized Platform}} for {{Community Governance}}},
  shorttitle = {Gov4git},
  year = {2023},
  month = mar,
  journal = {Microsoft Research},
  urldate = {2024-06-13},
  abstract = {E. Glen Weyl, Kasia Sitkiewicz, Petar Maymounkov Open-source software, as exemplified by the communities that have formed around the ``git'' protocol for collaborative software development and version management, is one of the great wonders of digital technology.~ Communal open-source projects have found applications well beyond collaborating on code. They have been used in various capacities [{\dots}]},
  langid = {american}
}

@article{gratzlLineUpVisualAnalysis2013,
  title = {{{LineUp}}: {{Visual Analysis}} of {{Multi-Attribute Rankings}}},
  shorttitle = {{{LineUp}}},
  author = {Gratzl, Samuel and Lex, Alexander and Gehlenborg, Nils and Pfister, Hanspeter and Streit, Marc},
  year = {2013},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {19},
  number = {12},
  pages = {2277--2286},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2013.173},
  urldate = {2022-01-14},
  abstract = {Rankings are a popular and universal approach to structuring otherwise unorganized collections of items by computing a rank for each item based on the value of one or more of its attributes. This allows us, for example, to prioritize tasks or to evaluate the performance of products relative to each other. While the visualization of a ranking itself is straightforward, its interpretation is not, because the rank of an item represents only a summary of a potentially complicated relationship between its attributes and those of the other items. It is also common that alternative rankings exist which need to be compared and analyzed to gain insight into how multiple heterogeneous attributes affect the rankings. Advanced visual exploration tools are needed to make this process efficient. In this paper we present a comprehensive analysis of requirements for the visualization of multi-attribute rankings. Based on these considerations, we propose LineUp - a novel and scalable visualization technique that uses bar charts. This interactive technique supports the ranking of items based on multiple heterogeneous attributes with different scales and semantics. It enables users to interactively combine attributes and flexibly refine parameters to explore the effect of changes in the attribute combination. This process can be employed to derive actionable insights as to which attributes of an item need to be modified in order for its rank to change. Additionally, through integration of slope graphs, LineUp can also be used to compare multiple alternative rankings on the same set of items, for example, over time or across different attribute combinations. We evaluate the effectiveness of the proposed multi-attribute visualization technique in a qualitative study. The study shows that users are able to successfully solve complex ranking tasks in a short period of time.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\3596QNI4\Gratzl et al. - 2013 - LineUp Visual Analysis of Multi-Attribute Ranking.pdf}
}

@article{greeneComparisonUsabilityVoting,
  title = {A {{Comparison}} of {{Usability Between Voting Methods}}},
  author = {Greene, Kristen K and Byrne, Michael D and Everett, Sarah P},
  pages = {7},
  abstract = {In order to know if new electronic voting systems truly represent a gain in usability, it is necessary to have information about the usability of more traditional voting methods. The usability of three different voting methods was evaluated using the metrics recommended by the National Institute of Standards and Technology: efficiency, effectiveness, and satisfaction. Participants voted with two different types of paper-based ballots (one openresponse ballot and one bubble ballot) and one mechanical lever machine. No significant differences in voting completion times or error rates were found between voting methods. However, large differences in satisfaction ratings indicated that participants were most satisfied with the bubble ballot and least satisfied with the lever machine. These data are an important step in addressing the need for baseline usability measures for existing voting systems.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\RFVTQCGY\Greene et al. - A Comparison of Usability Between Voting Methods.pdf}
}

@article{guayAssessmentSituationalIntrinsic2001,
  title = {On the {{Assessment}} of {{Situational Intrinsic}} and {{Extrinsic Motivation}}: {{The Situational Motivation Scale}} ({{SIMS}})},
  author = {Guay, Frederic and Vallerand, Robert J and Blanchard, Celine},
  year = {2001},
  journal = {Motivation and Emotion},
  pages = {41},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\DSY534UZ\Guay et al. - 2001 - On the Assessment of Situational Intrinsic and Ext.pdf}
}

@incollection{hartDevelopmentNASATLXTask1988,
  title = {Development of {{NASA-TLX}} ({{Task Load Index}}): {{Results}} of {{Empirical}} and {{Theoretical Research}}},
  shorttitle = {Development of {{NASA-TLX}} ({{Task Load Index}})},
  booktitle = {Advances in {{Psychology}}},
  author = {Hart, Sandra G. and Staveland, Lowell E.},
  year = {1988},
  volume = {52},
  pages = {139--183},
  publisher = {Elsevier},
  doi = {10.1016/S0166-4115(08)62386-9},
  urldate = {2022-05-31},
  abstract = {T h e results of a multi-year research program to identiJy the Jactors asaoriaied with variations i n subjective workload uizthin and betweerr different types OJ tasks are reviewed. Subjecizve evalualions oJ 10 utorkload-related factors were obtained J r o m 16 different urperzments. The ezperimental tasks included simple cogn i t i w and manual control tasks, complez laboratory and supervisory control tasks, and aircraJi simulation. Task-, behavior-, and subject-related correlates OJ subjeciive workload ezperiences wried as a Junction oJ difficulty manipulations within experiments, different sources OJ workload between experiments. and individual differences in workload definition. A multi-dimensional rating scale is proposed in which inJormation about the magnitude and sources oJ six workload-related factors are combined io derive a sensitzve and reliable estimate of workload.},
  isbn = {978-0-444-70388-0},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\XYF7TC7K\Hart and Staveland - 1988 - Development of NASA-TLX (Task Load Index) Results.pdf}
}

@article{hartNasaTaskLoadIndex2006,
  title = {Nasa-{{Task Load Index}} ({{NASA-TLX}}); 20 {{Years Later}}},
  author = {Hart, Sandra G.},
  year = {2006},
  month = oct,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {50},
  number = {9},
  pages = {904--908},
  publisher = {SAGE Publications Inc},
  issn = {2169-5067},
  doi = {10.1177/154193120605000909},
  urldate = {2022-05-31},
  abstract = {NASA-TLX is a multi-dimensional scale designed to obtain workload estimates from one or more operators while they are performing a task or immediately afterwards. The years of research that preceded subscale selection and the weighted averaging approach resulted in a tool that has proven to be reasonably easy to use and reliably sensitive to experimentally important manipulations over the past 20 years. Its use has spread far beyond its original application (aviation), focus (crew complement), and language (English). This survey of 550 studies in which NASA-TLX was used or reviewed was undertaken to provide a resource for a new generation of users. The goal was to summarize the environments in which it has been applied, the types of activities the raters performed, other variables that were measured that did (or did not) covary, methodological issues, and lessons learned},
  file = {C:\Users\tcheng\Zotero\storage\F3HDBI6P\Hart - 2006 - Nasa-Task Load Index (NASA-TLX); 20 Years Later.pdf}
}

@article{hauserIntensityMeasuresConsumer1980,
  title = {Intensity {{Measures}} of {{Consumer Preference}}},
  author = {Hauser, John R. and Shugan, Steven M.},
  year = {1980},
  journal = {Operations Research},
  volume = {28},
  number = {2},
  eprint = {170448},
  eprinttype = {jstor},
  pages = {278--320},
  publisher = {INFORMS},
  issn = {0030-364X},
  urldate = {2022-07-27},
  abstract = {To design successful new products and services, managers need to measure consumer preferences relative to product attributes. Many existing methods use ordinal measures. Intensity measures have the potential to provide more information per question, thus allowing more accurate models or fewer consumer questions (lower survey cost, less consumer wearout). To exploit this potential, researchers must be able to identify how consumers react to these questions and must be able to estimate intensity-based preference functions. This paper provides a general structure for using intensity measures for estimating consumer preference functions. Within the structure: (1) alternative measurement theories are reviewed, (2) axioms for developing testable implications of each theory are provided, (3) statistical tests to test these implications and distinguish which theory describes how consumers are using the intensity measures are developed, (4) functional forms appropriate for the preference functions implied by each theory are derived, and (5) procedures to estimate the parameters of these preference functions are provided. Based on these results, a practical procedure, implemented by an interactive computer package, to measure preference functions in a market research environment is developed. An empirical case illustrates how the statistical tests and estimation procedures are used to aid in the design of new telecommunications devices. Empirical results suggest the majority of consumers can provide intensity judgments. The intensity-based estimation procedures do better on several criteria than ordinal estimation.},
  file = {C:\Users\tcheng\Zotero\storage\ZLCNV8S3\Hauser and Shugan - 1980 - Intensity Measures of Consumer Preference.pdf}
}

@article{hauserIntensityMeasuresConsumer1980a,
  title = {Intensity {{Measures}} of {{Consumer Preference}}},
  author = {Hauser, John R. and Shugan, Steven M.},
  year = {1980},
  journal = {Operations Research},
  volume = {28},
  number = {2},
  eprint = {170448},
  eprinttype = {jstor},
  pages = {278--320},
  publisher = {INFORMS},
  issn = {0030-364X},
  urldate = {2023-12-16},
  abstract = {To design successful new products and services, managers need to measure consumer preferences relative to product attributes. Many existing methods use ordinal measures. Intensity measures have the potential to provide more information per question, thus allowing more accurate models or fewer consumer questions (lower survey cost, less consumer wearout). To exploit this potential, researchers must be able to identify how consumers react to these questions and must be able to estimate intensity-based preference functions. This paper provides a general structure for using intensity measures for estimating consumer preference functions. Within the structure: (1) alternative measurement theories are reviewed, (2) axioms for developing testable implications of each theory are provided, (3) statistical tests to test these implications and distinguish which theory describes how consumers are using the intensity measures are developed, (4) functional forms appropriate for the preference functions implied by each theory are derived, and (5) procedures to estimate the parameters of these preference functions are provided. Based on these results, a practical procedure, implemented by an interactive computer package, to measure preference functions in a market research environment is developed. An empirical case illustrates how the statistical tests and estimation procedures are used to aid in the design of new telecommunications devices. Empirical results suggest the majority of consumers can provide intensity judgments. The intensity-based estimation procedures do better on several criteria than ordinal estimation.}
}

@article{herrnsonEVALUATIONMARYLANDNEW2003,
  title = {{{AN EVALUATION OF MARYLAND}}'{{S NEW VOTING MACHINE}}},
  author = {Herrnson, Paul S. and Bederson, Benjamin B.},
  year = {2003},
  month = jan,
  urldate = {2023-12-16},
  abstract = {Four counties in Maryland used new touch screen voting machines in the 2002 elections, replacing their mechanical lever and punch card voting systems with the AccuVote-TS touch screen voting machine manufactured by Diebold Election Systems. The Center for American Politics and Citizenship (CAPC) and the Human-Computer Interaction Lab (HCIL) at the University of Maryland conducted an exit poll in Montgomery and Prince George's counties to evaluate the performance of the new voting machines. In this second of two reports prepared by CAPC and HCIL on the new voting machines, we found that most voters like the new voting machines and trust them to accurately record their votes. However, a significant number of voters still have concerns about the new machines, many needed help using them, and some continue to report technical problems with the machines. Voters who do not frequently use computers or have not attended college had the most difficulty using the machines. Major Findings: * Seven percent of voters felt that the touch screen voting machine was not easy to use, compared to 93 percent who felt it was easy to use or held a neutral opinion. * Nine percent of voters did not trust the touch screen voting machine, compared to with 91 percent who did. Only 70 percent trusted the mechanical lever or punch card system they previously used. * Three percent of voters reported encountering technical problems with the new machines. * Nine percent of the voters asked for and 17 percent received assistance using the new machine. * More than one-quarter of the voters who use computers once a month or less received assistance using the voting machine. * One-third of voters who have not attended college received assistance using the voting machine. * Voters in Prince George's County found the election judges to be more helpful than did voters in Montgomery County. Four counties in Maryland used new touch screen voting machines in the 2002 elections. Alleghany, Dorchester, Montgomery, and Prince George's replaced their mechanical lever and punch card voting systems with the AccuVote-TS touch screen voting machine manufactured by Diebold Election Systems. All 24 of Maryland's counties will purchase AccuVote-TS voting machines by 2006. The University of Maryland conducted an exit poll in Montgomery and Prince George's Counties to assess the performance of the new voting machine. Our sample included 1,276 respondents from 22 precincts in the two counties. The response rate was 74.6 percent. (UMIACS-TR-2002-107) (HCIL-TR-2002-25)},
  langid = {american},
  file = {C:\Users\tcheng\Zotero\storage\UJZQG24L\Herrnson and Bederson - 2003 - AN EVALUATION OF MARYLAND'S NEW VOTING MACHINE.pdf}
}

@article{heymanRespondentfriendlyMethodRanking2016,
  title = {A {{Respondent-friendly Method}} of {{Ranking Long Lists}}},
  author = {Heyman, James and Sailors, John},
  year = {2016},
  month = sep,
  journal = {International Journal of Market Research},
  volume = {58},
  number = {5},
  pages = {693--710},
  issn = {1470-7853, 2515-2173},
  doi = {10.2501/IJMR-2016-001},
  urldate = {2022-01-14},
  abstract = {This article illustrates a consumer-friendly approach to preference elicitation over large choice sets that overcomes limitations of rating, full-list ranking, conjoint, and choice-based approaches. This approach, HLm, requires respondents to identify the top and bottom m items from an overall list. Across respondents, the number of times an item appears in participants' L (low) list is subtracted from the number of times it appears in participants' H (high) list. These net scores are then used to order the total list. We illustrate the approach in three experiments, demonstrating that it compares favorably to familiar methods while being much less demanding on survey participants. Experiment 1 had participants alphabetize words, suggesting the HLm method is easier than full ranking but less accurate if m does not increase with increases in list length. The objective of experiment 2 was to order U.S. states by populations. In this domain, where knowledge was imperfect, HLm outperformed full ranking. Experiment 3 involved eliciting respondents' personal tastes for fruit. HLm resulted in a final ranking that correlated highly with max-diff scaling. We argue that HLm is a viable method for obtaining aggregate order of preferences across large numbers of alternatives.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\WWYWHSSA\Heyman and Sailors - 2016 - A Respondent-friendly Method of Ranking Long Lists.pdf}
}

@article{holzInteractionStylesContext2021,
  title = {Interaction {{Styles}} in {{Context}}: {{Comparing Drag-and-Drop}}, {{Point-and-Touch}}, and {{Touch}} in a {{Mobile Spelling Game}}},
  shorttitle = {Interaction {{Styles}} in {{Context}}},
  author = {Holz, Heiko and Meurers, Detmar},
  year = {2021},
  month = may,
  journal = {International Journal of Human--Computer Interaction},
  volume = {37},
  number = {9},
  pages = {835--850},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2020.1848160},
  urldate = {2022-08-01},
  abstract = {The choice of appropriate interaction style for children's application has been a divisive subject of debate. For example, drag-and-drop is often said to perform worse than point-and-click in educational applications -- and vice versa. In this article, we argue for the need to choose the interaction style in context, considering a range of factors. We compare drag-and-drop, point-and-touch, and simple touch for selecting letters to form words in a spelling line as part of an educational spelling game. We evaluate the perceived workload, user experience, preference, and writing times of twenty-five children (8--11 years), eight of whom were dyslexic. We found that touch received better ratings and was ranked highest most often on all subscales compared to drag-and-drop and point-and-touch. Children needed less time using touch and 68\% chose it as their favorite interaction style. We also found small advantages for drag-and-drop over point-and-touch, which runs counter to some recent recommendations. This becomes particularly clear when using ranking responses, which support a particularly fine-grained picture.},
  file = {C:\Users\tcheng\Zotero\storage\TUSPTHJY\10447318.2020.html}
}

@misc{HowAssessUsability,
  title = {How to {{Assess}} the {{Usability Metrics}} of {{E-Voting Schemes}} {\textbar} {{SpringerLink}}},
  urldate = {2022-01-14},
  howpublished = {https://link.springer.com/chapter/10.1007/978-3-030-43725-1\_18},
  file = {C:\Users\tcheng\Zotero\storage\K7KWFQRN\978-3-030-43725-1_18.html}
}

@misc{HowWeClassify,
  title = {How {{Do We Classify Charities}}?},
  journal = {Charity Navigator},
  urldate = {2022-10-19},
  abstract = {Learn more about how charities are classified by the Internal Revenue Service and NTEE codes.},
  howpublished = {http://www.charitynavigator.org/index.cfm?bay=content.view\&cpid=34},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\VAIZ64WS\index.html}
}

@article{huangMeasuringEffectivenessGraph2009,
  title = {Measuring {{Effectiveness}} of {{Graph Visualizations}}: {{A Cognitive Load Perspective}}},
  shorttitle = {Measuring {{Effectiveness}} of {{Graph Visualizations}}},
  author = {Huang, Weidong and Eades, Peter and Hong, Seok-Hee},
  year = {2009},
  month = sep,
  journal = {Information Visualization},
  volume = {8},
  number = {3},
  pages = {139--152},
  publisher = {SAGE Publications},
  issn = {1473-8716},
  doi = {10.1057/ivs.2009.10},
  urldate = {2023-12-15},
  abstract = {Graph visualizations are typically evaluated by comparing their differences in effectiveness, measured by task performance such as response time and accuracy. Such performance-based measures have proved to be useful in their own right. There are some situations, however, where the performance measures alone may not be sensitive enough to detect differences. This limitation can be seen from the fact that the graph viewer may achieve the same level of performance by devoting different amounts of cognitive effort. In addition, it is not often that individual performance measures are consistently in favor of a particular visualization. This makes design and evaluation difficult in choosing one visualization over another. In an attempt to overcome the above-mentioned limitations, we measure the effectiveness of graph visualizations from a cognitive load perspective. Human memory as an information processing system and recent results from cognitive load research are reviewed first. The construct of cognitive load in the context of graph visualization is proposed and discussed. A model of user task performance, mental effort and cognitive load is proposed thereafter to further reveal the interacting relations between these three concepts. A cognitive load measure called mental effort is introduced and this measure is further combined with traditional performance measures into a single multi-dimensional measure called visualization efficiency. The proposed model and measurements are tested in a user study for validity. Implications of the cognitive load considerations in graph visualization are discussed.},
  file = {C:\Users\tcheng\Zotero\storage\Z46K38QS\Huang et al. - 2009 - Measuring Effectiveness of Graph Visualizations A.pdf}
}

@article{huiEffectsCultureResponse1989,
  title = {Effects of {{Culture}} and {{Response Format}} on {{Extreme Response Style}}},
  author = {Hui, C. Harry and Triandis, Harry C.},
  year = {1989},
  month = sep,
  journal = {Journal of Cross-Cultural Psychology},
  volume = {20},
  number = {3},
  pages = {296--309},
  publisher = {SAGE Publications Inc},
  issn = {0022-0221},
  doi = {10.1177/0022022189203004},
  urldate = {2022-08-02},
  abstract = {Do cultural and ethnic groups differ in their extreme response style? To answer this question, Hispanic and non-Hispanic subjects were asked to respond to a questionnaire on 5-point or 10-point scales. As predicted, Hispanics were found to exhibit a stronger tendency for extreme checking (about half the time, on the average) than non-Hispanic, but only when the 5-point scales were used. Use of 10-point scales reduced the extreme responses of the Hispanics to the level of non-Hispanics. Extreme responses of non-Hispanics were not affected by the scales. Implications of the findings for social research are discussed.},
  file = {C:\Users\tcheng\Zotero\storage\YV5NEU9D\Hui and Triandis - 1989 - Effects of Culture and Response Format on Extreme .pdf}
}

@misc{ImprovingUsabilityUX,
  title = {Improving the {{Usability}} and {{UX}} of the {{Swiss Internet Voting Interface}} {\textbar} {{Proceedings}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/3313831.3376769},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\3YNH26R3\\3313831.html;C\:\\Users\\tcheng\\Zotero\\storage\\W6AGA97U\\3313831.html}
}

@misc{InternetVotingProceedings,
  title = {Internet Voting {\textbar} {{Proceedings}} of the 7th {{International Conference}} on {{Theory}} and {{Practice}} of {{Electronic Governance}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/2591888.2591953},
  file = {C:\Users\tcheng\Zotero\storage\XDNSV3CJ\2591888.html}
}

@misc{InvestigatingUsabilityUser,
  title = {Investigating {{Usability}} and {{User Experience}} of {{Individually Verifiable Internet Voting Schemes}} {\textbar} {{ACM Transactions}} on {{Computer-Human Interaction}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/3459604},
  file = {C:\Users\tcheng\Zotero\storage\5W7YGS7U\3459604.html}
}

@misc{IVoteCHI04,
  title = {I-{{Vote}} {\textbar} {{CHI}} '04 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/985921.986181},
  file = {C:\Users\tcheng\Zotero\storage\ENFPCAA5\985921.html}
}

@article{iyengarWhenChoiceDemotivating2000,
  title = {When Choice Is Demotivating: {{Can}} One Desire Too Much of a Good Thing?},
  author = {Iyengar, Sheena S and Lepper, Mark R},
  year = {2000},
  journal = {Journal of personality and social psychology},
  volume = {79},
  number = {6},
  pages = {995},
  publisher = {American Psychological Association}
}

@book{jackoHumancomputerInteractionHandbook2012,
  title = {The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies, and Emerging Applications},
  shorttitle = {The Human-Computer Interaction Handbook},
  editor = {Jacko, Julie A.},
  year = {2012},
  series = {Human Factors and Ergonomics},
  edition = {3. ed},
  publisher = {CRC Press, Taylor \& Francis},
  address = {Boca Raton, Fla.},
  abstract = {Humans in HCI -- Computers in HCI -- Designing Human-Computer Interactions -- Application Design, Domain-Specific design -- Designing for Diversity -- Development Prozess -- Requirements Specification -- Design and Development -- Testing, Evalution, and Technology Transfer -- Emerging Phenomena in HCI. Interaktionsgestaltung -- Interaktionsdesign -- Interaction Design. (Display -- Sensor -- Haptik -- wearable computer -- mobile information devices -- human-centerd design)},
  isbn = {978-1-4398-2944-8 978-1-4398-2943-1},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\ZN3BL6UA\Jacko - 2012 - The human-computer interaction handbook fundament.pdf}
}

@article{jansenEffectQuestionnaireLayout1985,
  title = {Effect of {{Questionnaire Layout}} and {{Size}} and {{Issue-Involvement}} on {{Response Rates}} in {{Mail Surveys}}},
  author = {Jansen, J. Harry},
  year = {1985},
  month = aug,
  journal = {Perceptual and Motor Skills},
  volume = {61},
  number = {1},
  pages = {139--142},
  publisher = {SAGE Publications Inc},
  issn = {0031-5125},
  doi = {10.2466/pms.1985.61.1.139},
  urldate = {2022-05-31},
  abstract = {A pilot study and an experiment on the design of questionnaires was implemented in two surveys. In the first pilot study (n = 856) layout and page-size were varied; in the experiment (n = 1360) only the page-size differed. Professional design of the questionnaire had no significant positive effect on the response rates. In the experiment questionnaires of the size 17 ? 24 cm had significant higher response rates than the smaller one (14.5 ? 21 cm). Questionnaires judged to be salient to or relevant to respondents' interests had a higher response rate.},
  file = {C:\Users\tcheng\Zotero\storage\EJ7YQEB5\Jansen - 1985 - Effect of Questionnaire Layout and Size and Issue-.pdf}
}

@phdthesis{kaczmirekHumanSurveyInteractionUsability2008,
  title = {{Human-Survey Interaction Usability and Nonresponse in Online Surveys}},
  author = {Kaczmirek, Lars},
  year = {2008},
  month = jun,
  abstract = {Response rates are a key quality indicator of surveys. The human-survey interaction framework developed in this book provides new insight in what makes respondents leave or complete an online survey. Many respondents suffer from difficulties when trying to answer survey questions. This results in omitted answers and abandoned questionnaires. Lars Kaczmirek explains how applied usability in surveys increases response rates. Here, central aspects addressed in the studies include error tolerance and useful feedback. Recommendations are drawn from seven studies and experiments. The results report on more than 33,000 respondents sampled from many different populations such as students, people above forty, visually impaired and blind people, and survey panel members. The results show that improved usability significantly boosts response rates and accessibility. This work clearly demonstrates that human-survey interaction is a cost-effective approach in the overall context of survey methodology.},
  langid = {ngerman},
  school = {Universit{\"a}t Mannheim},
  file = {C:\Users\tcheng\Zotero\storage\Z3NHCL2C\Kaczmirek - Human-Survey Interaction Usability and Nonresponse.pdf}
}

@article{kammounGenerativeAdversarialNetworks2022,
  title = {Generative {{Adversarial Networks}} for Face Generation: {{A}} Survey},
  shorttitle = {Generative {{Adversarial Networks}} for Face Generation},
  author = {Kammoun, Amina and Slama, Rim and Tabia, Hedi and Ouni, Tarek and Abid, Mohmed},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  pages = {1122445.1122456},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/1122445.1122456},
  urldate = {2022-11-11},
  abstract = {Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\QYCEVVX7\Kammoun et al. - 2022 - Generative Adversarial Networks for face generatio.pdf}
}

@article{kierujVariationsResponseStyle2010,
  title = {Variations in {{Response Style Behavior}} by {{Response Scale Format}} in {{Attitude Research}}},
  author = {Kieruj, N. D. and Moors, G.},
  year = {2010},
  month = sep,
  journal = {International Journal of Public Opinion Research},
  volume = {22},
  number = {3},
  pages = {320--342},
  issn = {0954-2892, 1471-6909},
  doi = {10.1093/ijpor/edq001},
  urldate = {2024-06-13},
  abstract = {Studies concerning the impact of the length of response scales on the measurement of attitudes have primarily focused on the method bias associated with question format. At the same time another line of research has focused on the issue of response styles that affect how respondents answer to attitude questions. So far, research has paid less attention to the issue of whether the length of the response scales is related to response styles. In this study, we explore if differences in length of the response scale (i.e., method factor) have differential effects in evoking extreme and midpoint response style behavior (i.e., style factor). Our hypotheses read as follows. As the number of response categories increases, we expect subjects to be more likely to exert extreme response style. Furthermore, we expect subjects to be more likely to adopt a midpoint response style when they are offered a middle response category. To investigate these hypotheses we developed a split ballot experiment in which the number of response categories is manipulated from 5 to 11 categories. Data are collected by a random sample, large-scale web survey which allows for random assignment to the experimental conditions. The results show clear evidence of extreme response style and moderate evidence of midpoint response style. Extreme response style is not affected by the length of response scales, whereas the exertion of midpoint response style only popped up in the longer scale versions.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\JDBASS6Q\Kieruj and Moors - 2010 - Variations in Response Style Behavior by Response .pdf}
}

@inproceedings{kimComparingDataChatbot2019,
  title = {Comparing {{Data}} from {{Chatbot}} and {{Web Surveys}}: {{Effects}} of {{Platform}} and {{Conversational Style}} on {{Survey Response Quality}}},
  shorttitle = {Comparing {{Data}} from {{Chatbot}} and {{Web Surveys}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Soomin and Lee, Joonhwan and Gweon, Gahgene},
  year = {2019},
  month = may,
  pages = {1--12},
  publisher = {ACM},
  address = {Glasgow Scotland Uk},
  doi = {10.1145/3290605.3300316},
  urldate = {2022-05-31},
  abstract = {This study aims to explore the feasibility of a text-based virtual agent as a new survey method to overcome the web survey's common response quality problems, which are caused by respondents' inattention. To this end, we conducted a 2 (platform: web vs. chatbot) {\texttimes} 2 (conversational style: formal vs. casual) experiment. We used satisficing theory to compare the responses' data quality. We found that the participants in the chatbot survey, as compared to those in the web survey, were more likely to produce differentiated responses and were less likely to satisfice; the chatbot survey thus resulted in higher-quality data. Moreover, when a casual conversational style is used, the participants were less likely to satisfice--although such effects were only found in the chatbot condition. These results imply that conversational interactivity occurs when a chat interface is accompanied by messages with effective tone. Based on an analysis of the qualitative responses, we also showed that a chatbot could perform part of a human interviewer's role by applying effective communication strategies.},
  isbn = {978-1-4503-5970-2},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\D3FP78NX\Kim et al. - 2019 - Comparing Data from Chatbot and Web Surveys Effec.pdf}
}

@article{kleinResponseSetsMeasurement2004,
  title = {Response {{Sets}} in the {{Measurement}} of {{Values}}: {{A Comparison}} of {{Rating}} and {{Ranking Procedures}}},
  shorttitle = {Response {{Sets}} in the {{Measurement}} of {{Values}}},
  author = {Klein, M. and Dulmer, H. and Ohr, D. and Quandt, M. and Rosar, U.},
  year = {2004},
  month = dec,
  journal = {International Journal of Public Opinion Research},
  volume = {16},
  number = {4},
  pages = {474--483},
  issn = {0954-2892, 1471-6909},
  doi = {10.1093/ijpor/edh041},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\352DJQEM\Klein et al. - 2004 - Response Sets in the Measurement of Values A Comp.pdf}
}

@incollection{krosnick2018measurement,
  title = {The Measurement of Attitudes},
  booktitle = {The Handbook of Attitudes},
  author = {Krosnick, Jon A and Judd, Charles M and Wittenbrink, Bernd},
  year = {2018},
  pages = {45--105},
  publisher = {Routledge}
}

@article{krosnickResponseStrategiesCoping1991,
  title = {Response Strategies for Coping with the Cognitive Demands of Attitude Measures in Surveys},
  author = {Krosnick, Jon A.},
  year = {1991},
  month = may,
  journal = {Applied Cognitive Psychology},
  volume = {5},
  number = {3},
  pages = {213--236},
  issn = {08884080, 10990720},
  doi = {10.1002/acp.2350050305},
  urldate = {2022-01-14},
  abstract = {This paper proposes that when optimally answeringa survey question would require substantial cognitive effort, some respondents simply provide a satisfactory answer instead. This behaviour, called satisficing, can take the form of either (1) incomplete or biased information retrieval and/or information integration, or (2) no information retrieval or integration at all. Satisficing may lead respondents to employ a variety of response strategies, including choosing the first response alternative that seems to constitute a reasonable answer, agreeing with an assertion made by a question, endorsing the status quo instead of endorsing social change, failing to differentiate among a set of diverse objects in ratings, saying `don't know' instead of reporting an opinion, and randomly choosing among the response alternatives offered. This paper specifies a wide range of factors that are likely to encourage satisficing,and reviews relevant evidence evaluating these speculations. Many useful directions for future research are suggested.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\5D2NNC62\Krosnick - 1991 - Response strategies for coping with the cognitive .pdf}
}

@inproceedings{kuleszaTellMeMore2012,
  title = {Tell Me More? The Effects of Mental Model Soundness on Personalizing an Intelligent Agent},
  shorttitle = {Tell Me More?},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Kwan, Irwin},
  year = {2012},
  month = may,
  series = {{{CHI}} '12},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2207676.2207678},
  urldate = {2022-01-14},
  abstract = {What does a user need to know to productively work with an intelligent agent? Intelligent agents and recommender systems are gaining widespread use, potentially creating a need for end users to understand how these systems operate in order to fix their agent's personalized behavior. This paper explores the effects of mental model soundness on such personalization by providing structural knowledge of a music recommender system in an empirical study. Our findings show that participants were able to quickly build sound mental models of the recommender system's reasoning, and that participants who most improved their mental models during the study were significantly more likely to make the recommender operate to their satisfaction. These results suggest that by helping end users understand a system's reasoning, intelligent agents may elicit more and better feedback, thus more closely aligning their output with each user's intentions.},
  isbn = {978-1-4503-1015-4},
  keywords = {debugging,intelligent agents,mental models,music,personalization,recommenders},
  file = {C:\Users\tcheng\Zotero\storage\FJB8494X\Kulesza et al. - 2012 - Tell me more the effects of mental model soundnes.pdf}
}

@article{kuligaVirtualRealityEmpirical2015,
  title = {Virtual Reality as an Empirical Research Tool --- {{Exploring}} User Experience in a Real Building and a Corresponding Virtual Model},
  author = {Kuliga, S.F. and Thrash, T. and Dalton, R.C. and H{\"o}lscher, C.},
  year = {2015},
  month = nov,
  journal = {Computers, Environment and Urban Systems},
  volume = {54},
  pages = {363--375},
  issn = {01989715},
  doi = {10.1016/j.compenvurbsys.2015.09.006},
  urldate = {2022-01-14},
  abstract = {Virtual reality (VR) allows for highly-detailed observations, accurate behavior measurements, and systematic environmental manipulations under controlled laboratory circumstances. It therefore has the potential to be a valuable research tool for studies in human--environment interaction, such as building usability studies and post- as well as pre-occupancy building evaluation in architectural research and practice.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\239HUERU\Kuliga et al. - 2015 - Virtual reality as an empirical research tool — Ex.pdf}
}

@inproceedings{lalley2018quadratic,
  title = {Quadratic Voting: {{How}} Mechanism Design Can Radicalize Democracy},
  booktitle = {{{AEA}} Papers and Proceedings},
  author = {Lalley, Steven P and Weyl, E Glen},
  year = {2018},
  volume = {108},
  pages = {33--37},
  file = {C:\Users\tcheng\Zotero\storage\EF8MERKV\Lalley and Weyl - 2018 - Quadratic Voting How Mechanism Design Can Radical.pdf}
}

@article{laskowskiImprovingUsabilityAccessibility2004,
  title = {Improving the {{Usability}} and {{Accessibility}} of {{Voting Systems}} and {{Products}}},
  author = {Laskowski, Sharon J. and Yen, James H. and Autry, Marguerite W. and Cuginin, J. and Killam, William H.},
  year = {2004},
  month = may,
  urldate = {2022-01-14},
  abstract = {The goal of this report is to describe how research and best practices from the human factors, human-machine and human-computer interaction, and usability engin},
  langid = {english},
  keywords = {qsi_to_cite},
  annotation = {Last Modified: 2017-02-19T20:02-05:00},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\PBSK7AKM\\Laskowski et al. - 2004 - Improving the Usability and Accessibility of Votin.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\Y473JZZ3\\improving-usability-and-accessibility-voting-systems-and-products.html}
}

@inproceedings{leeUniversalDesignBallot2016,
  title = {Universal {{Design Ballot Interfaces}} on {{Voting Performance}} and {{Satisfaction}} of {{Voters}} with and without {{Vision Loss}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lee, Seunghyun "Tina" and Liu, Yilin Elaine and Ruzic, Ljilja and Sanford, Jon},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {4861--4871},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858567},
  urldate = {2023-12-15},
  abstract = {Voting is a glocalized event across countries, states and municipalities in which individuals of all abilities want to participate. To enable people with disabilities to participate accessible voting is typically implemented by adding assistive technologies to electronic voting machines to accommodate people with disabilities. To overcome the complexities and inequities in this practice, two interfaces, EZ Ballot, which uses a linear yes/no input system for all selections, and QUICK Ballot, which provides random access voting through direct selection, were designed to provide one system for all voters. This paper reports efficacy testing of both interfaces. The study demonstrated that voters with a range of visual abilities were able to use both ballots independently. While non-sighted voters made fewer errors on the linear ballot (EZ Ballot), partially-sighted and sighted voters completed the random access ballot (QUICK Ballot) in less time. In addition, a higher percentage of non-sighted participants preferred the linear ballot, and a higher percentage of sighted participants preferred the random ballot.},
  isbn = {978-1-4503-3362-7},
  keywords = {effectiveness,efficiency,subjective usability,universal design,voting},
  file = {C:\Users\tcheng\Zotero\storage\BR83QRVT\Lee et al. - 2016 - Universal Design Ballot Interfaces on Voting Perfo.pdf}
}

@article{lewisSystemUsabilityScale2018,
  title = {The {{System Usability Scale}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {The {{System Usability Scale}}},
  author = {Lewis, James R.},
  year = {2018},
  month = jul,
  journal = {International Journal of Human--Computer Interaction},
  volume = {34},
  number = {7},
  pages = {577--590},
  issn = {1044-7318, 1532-7590},
  doi = {10.1080/10447318.2018.1455307},
  urldate = {2022-05-31},
  langid = {english},
  keywords = {Credit scoring systems,Perceived usability,Questionnaire design,Questionnaires -- Data processing,standardized usability scale,Statistical reliability,SUS,System Usability,User-centered system design},
  file = {C:\Users\tcheng\Zotero\storage\DTIM3R7G\Lewis - 2018 - The System Usability Scale Past, Present, and Fut.pdf}
}

@book{lichtensteinConstructionPreference2006,
  title = {The Construction of Preference},
  editor = {Lichtenstein, Sarah and Slovic, Paul},
  year = {2006},
  edition = {1. publ},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  isbn = {978-0-521-54220-3 978-0-521-83428-5},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\H46E8WXA\Lichtenstein and Slovic - 2006 - The construction of preference.pdf}
}

@article{likertTechniqueMeasurementAttitudes1932,
  title = {A Technique for the Measurement of Attitudes},
  author = {Likert, R.},
  year = {1932},
  journal = {Archives of Psychology},
  volume = {22  140},
  pages = {55--55},
  abstract = {The project conceived in 1929 by Gardner Murphy and the writer aimed first to present a wide array of problems having to do with five major "attitude areas"---international relations, race relations, economic conflict, political conflict, and religion. The kind of questionnaire material falls into four classes: yes-no, multiple choice, propositions to be responded to by degrees of approval, and a series of brief newspaper narratives to be approved or disapproved in various degrees. The monograph aims to describe a technique rather than to give results. The appendix, covering ten pages, shows the method of constructing an attitude scale. A bibliography is also given. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\tcheng\Zotero\storage\6LKNH3CY\1933-01885-001.html}
}

@inproceedings{lohseQuantifyingEffectUser1998,
  title = {Quantifying the Effect of User Interface Design Features on Cyberstore Traffic and Sales},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lohse, Gerald L. and Spiller, Peter},
  year = {1998},
  month = jan,
  series = {{{CHI}} '98},
  pages = {211--218},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  address = {USA},
  doi = {10.1145/274644.274675},
  urldate = {2022-01-14},
  isbn = {978-0-201-30987-4},
  keywords = {economic value,electronic commerce,Internet retail store design,marketing,regression analysis,shopping,WWW},
  file = {C:\Users\tcheng\Zotero\storage\I6AVZYJD\Lohse and Spiller - 1998 - Quantifying the effect of user interface design fe.pdf}
}

@article{maoBetterHumanComputation2013,
  title = {Better {{Human Computation Through Principled Voting}}},
  author = {Mao, Andrew and Procaccia, Ariel and Chen, Yiling},
  year = {2013},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {27},
  number = {1},
  pages = {1142--1148},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v27i1.8460},
  urldate = {2022-07-18},
  abstract = {Designers of human computation systms often face the need to aggregate noisy information provided by multiple people. While voting is often used for this purpose, the choice of voting method is typically not principled. We conduct extensive experiments on Amazon Mechanical Turk to better understand how different voting rules perform in practice. Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict. Our short-term goal is to motivate the design of better human computation systems; our long-term goal is to spark an interaction between researchers in (computational) social choice and human computation.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\6W6S2CPT\Mao et al. - 2013 - Better Human Computation Through Principled Voting.pdf}
}

@incollection{markyImprovingUsabilityUX2020,
  title = {Improving the {{Usability}} and {{UX}} of the {{Swiss Internet Voting Interface}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Marky, Karola and Zimmermann, Verena and Funk, Markus and Daubert, J{\"o}rg and Bleck, Kira and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  month = apr,
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2022-01-14},
  abstract = {Up to 20\% of residential votes and up to 70\% of absentee votes in Switzerland are cast online. The Swiss system aims to provide individual verifiability by different verification codes. The voters have to carry out verification on their own, making the usability and UX of the interface of great importance. To improve the usability, we first performed an evaluation with 12 human-computer interaction experts to uncover usability weaknesses of the Swiss Internet voting interface. Based on the experts' findings, related work, and an exploratory user study with 36 participants, we propose a redesign that we evaluated in a user study with 49 participants. Our study confirmed that the redesign indeed improves the detection of incorrect votes by 33\% and increases the trust and understanding of the voters. Our studies furthermore contribute important lessons for designing verifiable e-voting systems in general.},
  isbn = {978-1-4503-6708-0},
  keywords = {e-voting,individual verifiability,usability evaluation},
  file = {C:\Users\tcheng\Zotero\storage\QA6SNL4N\Marky et al. - 2020 - Improving the Usability and UX of the Swiss Intern.pdf}
}

@article{markyInvestigatingUsabilityUser2021,
  title = {Investigating {{Usability}} and {{User Experience}} of {{Individually Verifiable Internet Voting Schemes}}},
  author = {Marky, Karola and Zollinger, Marie-Laure and Roenne, Peter and Ryan, Peter Y. A. and Grube, Tim and Kunze, Kai},
  year = {2021},
  month = sep,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {28},
  number = {5},
  pages = {30:1--30:36},
  issn = {1073-0516},
  doi = {10.1145/3459604},
  urldate = {2022-01-14},
  abstract = {Internet voting can afford more inclusive and inexpensive elections. The flip side is that the integrity of the election can be compromised by adversarial attacks and malfunctioning voting infrastructure. Individual verifiability aims to protect against such risks by letting voters verify that their votes are correctly registered in the electronic ballot box. Therefore, voters need to carry out additional tasks making human factors crucial for security. In this article, we establish a categorization of individually verifiable Internet voting schemes based on voter interactions. For each category in our proposed categorization, we evaluate a voting scheme in a user study with a total of 100 participants. In our study, we assessed usability, user experience, trust, and further qualitative data to gain deeper insights into voting schemes. Based on our results, we conclude with recommendations for developers and policymakers to inform the choices and design of individually verifiable Internet voting schemes.},
  keywords = {E-Voting,human factors,individual verifiability,Internet voting},
  file = {C:\Users\tcheng\Zotero\storage\KHC726XR\Marky et al. - 2021 - Investigating Usability and User Experience of Ind.pdf}
}

@article{marvelEffectiveInterfaceDesigns2020,
  title = {Towards {{Effective Interface Designs}} for {{Collaborative HRI}} in {{Manufacturing}}: {{Metrics}} and {{Measures}}},
  shorttitle = {Towards {{Effective Interface Designs}} for {{Collaborative HRI}} in {{Manufacturing}}},
  author = {Marvel, Jeremy A. and Bagchi, Shelly and Zimmerman, Megan and Antonishek, Brian},
  year = {2020},
  month = may,
  journal = {ACM Transactions on Human-Robot Interaction},
  volume = {9},
  number = {4},
  pages = {25:1--25:55},
  doi = {10.1145/3385009},
  urldate = {2022-01-14},
  abstract = {We present a comprehensive framework and test methodology for the evaluation of human-machine interfaces (HMI) and human-robot interactions (HRI) in collaborative manufacturing applications. An overview of the challenges that face current- and next-generation collaborative robot systems is presented, specifically focused on the interactions between man and machine, and a series of objectively quantitative and subjectively qualitative metrics are given to guide the development and assessment of interfaces and interactions. A generalized set of guidelines for the design of HMI is also proposed to address these challenges and thereby enable effective and intuitive diagnostics and error corrections when process failures occur. These guidelines are aimed at aiding researchers in developing effective interface and interaction technologies, maximizing operator situation awareness in human-robot collaborative manufacturing teams, promoting effective process and system diagnostics reporting, and enabling faster responses to equipment or application errors.},
  keywords = {benchmarking,data sets,peformance measures,repeatability studies,Test methods and metrics,use cases},
  file = {C:\Users\tcheng\Zotero\storage\MM7BMMN9\Marvel et al. - 2020 - Towards Effective Interface Designs for Collaborat.pdf}
}

@article{monkEffectInterruptionDuration2008,
  title = {The Effect of Interruption Duration and Demand on Resuming Suspended Goals.},
  author = {Monk, Christopher A. and Trafton, J. Gregory and {Boehm-Davis}, Deborah A.},
  year = {2008},
  journal = {Journal of Experimental Psychology: Applied},
  volume = {14},
  number = {4},
  pages = {299--313},
  issn = {1939-2192, 1076-898X},
  doi = {10.1037/a0014402},
  urldate = {2022-06-25},
  abstract = {The time to resume task goals after an interruption varied depending on the duration and cognitive demand of interruptions, as predicted by the memory for goals model (Altmann \& Trafton, 2002). Three experiments using an interleaved tasks interruption paradigm showed that longer and more demanding interruptions led to longer resumption times in a hierarchical, interactive task. The resumption time profile for durations up to 1 min supported the role of decay in defining resumption costs, and the interaction between duration and demand supported the importance of goal rehearsal in mitigating decay. These findings supported the memory for goals model, and had practical implications for context where tasks are frequently interleaved such as office settings, driving, emergency rooms, and aircraft cockpits.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\U4PDCF4T\Monk et al. - 2008 - The effect of interruption duration and demand on .pdf}
}

@book{moroneyQuestionnaireDesignHow2019,
  title = {Questionnaire {{Design}}: {{How}} to {{Ask}} the {{Right Questions}} of the {{Right People}} at the {{Right Time}} to {{Get}} the {{Information You Need}}},
  shorttitle = {Questionnaire {{Design}}},
  author = {Moroney, William F. and Cameron, Joyce A.},
  year = {2019},
  month = feb,
  publisher = {{Human Factors and Ergonomics Society}},
  isbn = {978-0-945289-55-5},
  langid = {english}
}

@article{mullenPublicInvolvementHealth1999,
  title = {Public Involvement in Health Care Priority Setting: An Overview of Methods for Eliciting Values: {{Public}} Involvement in Health Care Priority Setting},
  shorttitle = {Public Involvement in Health Care Priority Setting},
  author = {Mullen, Penelope M.},
  year = {1999},
  month = dec,
  journal = {Health Expectations},
  volume = {2},
  number = {4},
  pages = {222--234},
  issn = {13696513},
  doi = {10.1046/j.1369-6513.1999.00062.x},
  urldate = {2022-07-27},
  abstract = {There is increasing interest, in the UK and elsewhere, in involving the public in health care priority setting. At the same time, however, there is evidence of lack of clarity about the objectives of some priority setting projects and also about the role of public involvement. Further, some projects display an apparent ignorance of both long-standing theoretical literature and practical experience of methodologies for eliciting values in health care and related {\textregistered}elds. After a brief examination of the context of health care priority setting and public involvement, this paper describes a range of dierent approaches to eliciting values. These approaches are critically examined on a number of dimensions including the type of choice allowed to respondents and the implications of aggregation of values across individuals. Factors which aect the appropriateness of the dierent techniques to speci{\textregistered}c applications are discussed. A check-list of questions to be asked when selecting techniques is presented.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\86YC4JYB\Mullen - 1999 - Public involvement in health care priority setting.pdf}
}

@article{naglerTradingBenefitsCosts2015,
  title = {Trading off the Benefits and Costs of Choice: {{Evidence}} from {{Australian}} Elections},
  shorttitle = {Trading off the Benefits and Costs of Choice},
  author = {Nagler, Matthew G.},
  year = {2015},
  month = jun,
  journal = {Journal of Economic Behavior \& Organization},
  volume = {114},
  pages = {1--12},
  issn = {01672681},
  doi = {10.1016/j.jebo.2015.03.004},
  urldate = {2022-05-30},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\SULNMKVZ\Nagler - 2015 - Trading off the benefits and costs of choice Evid.pdf}
}

@article{naylor2017first,
  title = {First Year Student Conceptions of Success: {{What}} Really Matters?},
  author = {Naylor, Ryan and others},
  year = {2017},
  journal = {Student Success},
  volume = {8},
  number = {2},
  pages = {9--19},
  publisher = {Student Success Journal}
}

@misc{NeighborVoteProceedings29th,
  title = {Neighbor-{{Vote}} {\textbar} {{Proceedings}} of the 29th {{ACM International Conference}} on {{Multimedia}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/3474085.3475641},
  file = {C:\Users\tcheng\Zotero\storage\5P5LUGK5\3474085.html}
}

@misc{NewWayVoting,
  title = {A {{New Way}} of {{Voting That Makes Zealotry Expensive}} - {{Bloomberg}}},
  urldate = {2023-12-16},
  howpublished = {https://www.bloomberg.com/news/articles/2019-05-01/a-new-way-of-voting-that-makes-zealotry-expensive}
}

@book{norman2013design,
  title = {The Design of Everyday Things},
  author = {Norman Donald, A},
  year = {2013},
  publisher = {MIT Press}
}

@article{novWhyPeopleTag2010,
  title = {Why Do People Tag?: Motivations for Photo Tagging},
  shorttitle = {Why Do People Tag?},
  author = {Nov, Oded and Ye, Chen},
  year = {2010},
  month = jul,
  journal = {Communications of the ACM},
  volume = {53},
  number = {7},
  pages = {128--131},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1785414.1785450},
  urldate = {2022-07-26},
  abstract = {Introduction                          Tagging, or using keywords to add metadata to shared content, is gaining much popularity in recent years. Tags are used to annotate various types of content, including images, videos, bookmarks, and blogs, through web-based systems such as Flickr, YouTube, del.icio.us, and Technorati, respectively. The popularity of tagging is attributed, at least in part, to the benefits users gain from effective sharing and from organization of very large amounts of information.             As tagging receives increasing attention in both research and business communities, studies have found that users vary substantially in their tag usage, and suggested several factors that motivate user tagging. However, to date no quantitative study has assessed the strength of the effects of each motivation on levels of tag usage. This is surprising, since user participation is critical to the sustainability of content sharing communities, and a collaborative tagging system cannot succeed without higher level of user contribution. In what follows, we address this gap, by studying the strength of relationships between several motivations and users' tagging levels on Flickr, a prominent Web 2.0 photo sharing community.             Currently, there are more than 35 million Flickr users, who have so far uploaded more 3 billion photos. Each Flickr user can upload images and make them viewable by self, by designated friends and family, or by all Flickr users. Flickr users can annotate images with tags - unstructured textual labels; and usually images are tagged only by the user who uploaded them.8 These tags make the images searchable by the uploading user, as well as by others. In addition, each user can designate other users as "contacts," people whose photos the user follows (contacts are often reciprocal).             To understand what underlies tagging, we need to find out what motivates sharing in online environments, and in particular, what motivates tagging. Furthermore, we need to measure the degree to which different motivations affect tagging activity. While some studies has identified individual-level motivations for tagging, other studies have looked solely at the social level, focusing on the social presence as a driver of tagging.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\Q9SXMCQV\Nov and Ye - 2010 - Why do people tag motivations for photo tagging.pdf}
}

@article{olschewskiTaxingCognitiveCapacities2018,
  title = {Taxing Cognitive Capacities Reduces Choice Consistency Rather than Preference: {{A}} Model-Based Test.},
  shorttitle = {Taxing Cognitive Capacities Reduces Choice Consistency Rather than Preference},
  author = {Olschewski, Sebastian and Rieskamp, J{\"o}rg and Scheibehenne, Benjamin},
  year = {2018},
  month = apr,
  journal = {Journal of Experimental Psychology: General},
  volume = {147},
  number = {4},
  pages = {462--484},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000403},
  urldate = {2022-06-10},
  abstract = {How do people make preferential choices in situations where their cognitive capacities are limited? Many studies link the manipulation of cognitive resources to qualitative changes in preferences. However, there is a widely overlooked alternative hypothesis, namely, that a reduction in cognitive capacities leads to an increase in choice inconsistency. We developed a mathematical model and followed a hierarchical Bayesian estimation approach to test to what extent a reduction in cognitive capacities leads to a shift in preference or an increase in choice inconsistency. Using a within-subject n-back task to manipulate cognitive load, we conducted three experiments across different choice domains: risky choice, temporal discounting, and strategic interaction. Across all three domains, results show that a reduction in cognitive capacities predominantly affected participants' level of choice consistency rather than their respective preference. These results hold on an individual and a group level. In sum, our approach and the mathematical model we used provide a rigorous and general test of how reduced cognitive capacities affect people's decision-making.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\CKE6A2QH\Olschewski et al. - 2018 - Taxing cognitive capacities reduces choice consist.pdf}
}

@book{olsonWaysKnowingHCI2014,
  title = {Ways of {{Knowing}} in {{HCI}}},
  editor = {Olson, Judith S. and Kellogg, Wendy A.},
  year = {2014},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4939-0378-8},
  urldate = {2024-06-13},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-1-4939-0377-1 978-1-4939-0378-8},
  langid = {english},
  keywords = {Big data,CSCW,Design as research,Digital traces of behavior,Ethnography,HCI,Human computer interaction,Qualitative and quantitative,Research methodology,Research methods,Social computing,Tutorials},
  file = {C:\Users\tcheng\Zotero\storage\86YPKEL8\Olson and Kellogg - 2014 - Ways of Knowing in HCI.pdf}
}

@inproceedings{oulasvirtaWhenMoreLess2009,
  title = {When More Is Less: The Paradox of Choice in Search Engine Use},
  shorttitle = {When More Is Less},
  booktitle = {Proceedings of the 32nd International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval - {{SIGIR}} '09},
  author = {Oulasvirta, Antti and Hukkinen, Janne P. and Schwartz, Barry},
  year = {2009},
  pages = {516},
  publisher = {ACM Press},
  address = {Boston, MA, USA},
  doi = {10.1145/1571941.1572030},
  urldate = {2022-05-30},
  abstract = {In numerous everyday domains, it has been demonstrated that increasing the number of options beyond a handful can lead to paralysis and poor choice and decrease satisfaction with the choice. Were this so-called paradox of choice to hold in search engine use, it would mean that increasing recall can actually work counter to user satisfaction if it implies choice from a more extensive set of result items. The existence of this effect was demonstrated in an experiment where users (N=24) were shown a search scenario and a query and were required to choose the best result item within 30 seconds. Having to choose from six results yielded both higher subjective satisfaction with the choice and greater confidence in its correctness than when there were 24 items on the results page. We discuss this finding in the wider context of ``choice architecture''---that is, how result presentation affects choice and satisfaction.},
  isbn = {978-1-60558-483-6},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\CBTGIGE6\Oulasvirta et al. - 2009 - When more is less the paradox of choice in search.pdf}
}

@inproceedings{oviattQuietInterfacesThat2006,
  title = {Quiet Interfaces That Help Students Think},
  booktitle = {Proceedings of the 19th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '06},
  author = {Oviatt, Sharon and Arthur, Alex and Cohen, Julia},
  year = {2006},
  pages = {191},
  publisher = {ACM Press},
  address = {Montreux, Switzerland},
  doi = {10.1145/1166253.1166284},
  urldate = {2022-05-31},
  abstract = {As technical as we have become, modern computing has not permeated many important areas of our lives, including mathematics education which still involves pencil and paper. In the present study, twenty high school geometry students varying in ability from low to high participated in a comparative assessment of math problem solving using existing pencil and paper work practice (PP), and three different interfaces: an Anoto-based digital stylus and paper interface (DP), pen tablet interface (PT), and graphical tablet interface (GT). Cognitive Load Theory correctly predicted that as interfaces departed more from familiar work practice (GT {$>$} PT {$>$} DP), students would experience greater cognitive load such that performance would deteriorate in speed, attentional focus, meta-cognitive control, correctness of problem solutions, and memory. In addition, low-performing students experienced elevated cognitive load, with the more challenging interfaces (GT, PT) disrupting their performance disproportionately more than higher performers. The present results indicate that Cognitive Load Theory provides a coherent and powerful basis for predicting the rank ordering of users' performance by type of interface. In the future, new interfaces for areas like education and mobile computing could benefit from designs that minimize users' load so performance is more adequately supported.},
  isbn = {978-1-59593-313-3},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\WPDV52N5\Oviatt et al. - 2006 - Quiet interfaces that help students think.pdf}
}

@inproceedings{pielotDidYouMisclick2024,
  title = {Did {{You Misclick}}? {{Reversing}} 5-{{Point Satisfaction Scales Causes Unintended Responses}}},
  shorttitle = {Did {{You Misclick}}?},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Pielot, Martin and Callegaro, Mario},
  year = {2024},
  month = may,
  pages = {1--7},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613904.3642397},
  urldate = {2024-06-13},
  isbn = {9798400703300},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\N55HFX5R\Pielot and Callegaro - 2024 - Did You Misclick Reversing 5-Point Satisfaction S.pdf}
}

@book{posner2018radical,
  title = {Radical Markets: {{Uprooting}} Capitalism and Democracy for a Just Society},
  author = {Posner, Eric A and Weyl, E Glen},
  year = {2018},
  publisher = {Princeton University Press}
}

@article{potakaWhatEyeDoesn2007,
  title = {What the Eye Doesn't See: {{A}} Feasibility Study to Evaluate Eye-Tracking Technology as a Tool for Paper-Based Questionnaire Development},
  author = {Potaka, Lyn},
  year = {2007},
  journal = {Proceedings from Quest},
  pages = {162--171},
  file = {C:\Users\tcheng\Zotero\storage\B54UMQ7B\Potaka - 2007 - What the eye doesn’t see A feasibility study to e.pdf}
}

@article{pulestonGAMEEXPERIMENTS2011,
  title = {{{THE GAME EXPERIMENTS}}},
  author = {Puleston, Jon and Sleep, Deborah},
  year = {2011},
  pages = {28},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\7RAKAZHY\Puleston and Sleep - 2011 - THE GAME EXPERIMENTS.pdf}
}

@misc{QuadraticVotingColorado,
  title = {Quadratic {{Voting}} in {{Colorado}}},
  journal = {RadicalxChange},
  urldate = {2024-06-13},
  abstract = {We are a community of activists, artists, entrepreneurs, and scholars committed to using mechanism design to inspire radical social change.},
  howpublished = {https://www.radicalxchange.org/wiki/colorado-qv/},
  langid = {english}
}

@misc{QuadraticVotingFrontend2022,
  title = {Quadratic {{Voting Frontend}}},
  year = {2022},
  month = jan,
  urldate = {2023-12-16},
  copyright = {MIT},
  howpublished = {Public Digital Innovation Space}
}

@article{quarfoot2017quadratic,
  title = {Quadratic Voting in the Wild: Real People, Real Votes},
  author = {Quarfoot, David and {von Kohorn}, Douglas and Slavin, Kevin and Sutherland, Rory and Goldstein, David and Konar, Ellen},
  year = {2017},
  journal = {Public Choice},
  volume = {172},
  number = {1-2},
  pages = {283--303},
  publisher = {Springer}
}

@article{ravendranUsabilityEvaluationTagbased2012,
  title = {Usability Evaluation of a Tag-Based Interface},
  author = {Ravendran, Rajinesh and MacColl, Ian and Docherty, Michael},
  year = {2012},
  month = aug,
  journal = {Journal of Usability Studies},
  volume = {7},
  number = {4},
  pages = {143--160},
  issn = {1931-3357},
  abstract = {In this study, we report the findings of a comparative usability evaluation of a tag-based interface and the present conventional interface in the Australian banking context. The tag-based interface is based on user-assigned tags to banking resources with support for different types of customization, while the conventional interface is based on standard HTML objects such as dropdown boxes, lists, and tables, with limited customization. A total of 30 online banking users between the ages of 21 to 50 participated in the study. Each participant carried out a set of tasks on both interfaces and completed a post-test usability questionnaire. Additional feedback was sought from the participant through a post-evaluation debriefing session. Efficiency, effectiveness, and user satisfaction were considered to evaluate the usability of the interfaces. The results of the evaluation show that the tag-based interface improved usability over the conventional interface in terms of user satisfaction in both online and mobile contexts. This outcome is particularly apparent in the mobile context among inexperienced users. We conclude that there is a potential for the tag-based interface to improve user satisfaction of online and mobile banking, and also to positively affect the adoption and acceptance of mobile banking, particularly in Australia.},
  keywords = {acceptance,adoption,Australia,customization,effectiveness,efficiency,interaction style,mobile banking,online banking,satisfaction,tags,usability},
  file = {C:\Users\tcheng\Zotero\storage\EIINF854\Ravendran et al. - 2012 - Usability evaluation of a tag-based interface.pdf}
}

@article{revillaTestingDifferentRank2018,
  title = {Testing Different Rank Order Question Layouts for {{PC}} and Smartphone Respondents},
  author = {Revilla, Melanie and Couper, Mick P.},
  year = {2018},
  month = nov,
  journal = {International Journal of Social Research Methodology},
  volume = {21},
  number = {6},
  pages = {695--712},
  issn = {1364-5579, 1464-5300},
  doi = {10.1080/13645579.2018.1471371},
  urldate = {2022-08-02},
  abstract = {We studied the impact of different layouts for rank order questions on respondent effort, data quality, and substantive results among PC and smartphone respondents, in an experiment in an opt-in online panel in Spain, using an order-by-click design. We experimentally varied the device, the number of columns, and, for smartphone respondents, the position of the `next' button in questions on trust in institutions.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\R4MEVP5K\Revilla and Couper - 2018 - Testing different rank order question layouts for .pdf}
}

@article{rintoulVisualAnimatedResponse,
  title = {Visual and Animated Response Formats in Web Surveys: {{Do}} They Produce Better Data, or Is It All Just Fun and Games?},
  author = {Rintoul, Duncan},
  pages = {126},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\97T7RREP\Rintoul - Visual and animated response formats in web survey.pdf}
}

@misc{rohrerDesirabilityStudiesMeasuring2008,
  title = {Desirability {{Studies}}: {{Measuring Aesthetic Response}} to {{Visual Designs}}},
  shorttitle = {Desirability {{Studies}}},
  author = {Rohrer, Christian P.},
  year = {2008},
  month = oct,
  journal = {XDStrategy},
  urldate = {2022-01-14},
  abstract = {Studying the emotional response to visual and content design is hard to do quantitiatively, but desirability studies provide one way to get there.},
  langid = {american},
  file = {C:\Users\tcheng\Zotero\storage\AAJ3DG2D\desirability-studies.html}
}

@book{saatyGroupDecisionMaking2013,
  title = {Group {{Decision Making}}: {{Drawing Out}} and {{Reconciling Differences}}},
  shorttitle = {Group {{Decision Making}}},
  author = {Saaty, Thomas L. and Peniwati, Kirti},
  year = {2013},
  month = nov,
  publisher = {RWS Publications},
  abstract = {When a group makes a decision, that decision carries a lot more weight than when just one person does it. Think of the founding fathers of the American constitution and how much power and influence their ideas have had in the entire world for more than two hundred years. Also think of gravity, a universal force brought about by an enormous number of minute particles that band together to make a universal law. Together, they create a massive force, a law of nature; alone they can barely be noticed. That is how our minds work by deciding together to create a power that transcends our individuality. Group decision making is a gift and an opportunity to create greater influence through the working together of many minds. This book shows how to use the Analytic Hierarchy Process for hierarchical decision making and the Analytic Network Process for decision making in networks with dependence and feedback in group decision making. Part I discusses the group and the decision and shows the importance of using a structured process, particularly for those high value decisions involving many powerful parties with different interests. It discusses how to facilitate a group decision, combine individual judgments and smooth differences to arrive at a decision that everyone can live with and get behind. Part II discusses the group in planning and how to draw out differences. Part III is about conflict resolution and Part IV is about how to address significant issues that come up in group decision making and shows that it is possible to construct an overall group preference.},
  googlebooks = {rGwXAgAAQBAJ},
  isbn = {978-1-888603-22-4},
  langid = {english}
}

@inproceedings{saatyPrinciplesAnalyticHierarchy1987,
  title = {Principles of the {{Analytic Hierarchy Process}}},
  booktitle = {Expert {{Judgment}} and {{Expert Systems}}},
  author = {Saaty, Thomas L.},
  editor = {Mumpower, Jeryl L. and Renn, Ortwin and Phillips, Lawrence D. and Uppuluri, V. R. R.},
  year = {1987},
  pages = {27--73},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-86679-1_3},
  abstract = {Cognitive psychologists have classified thinking into two types. This division has gone by many names. Aristotle referred to it as active versus passive reason [35]; Freud [10] as secondary versus primary process thinking; and Hobbes [14] as thought with or without ``designe.'' More recently the division has been referred to as directed versus autistic thinking [4] and operant versus respondent thought [16]. The terms Varendinck [33] uses for the division may be most familiar to the layman. He noted the classification as one between conscious and foreconscious or affective thought. Adopting this familiar terminology, conscious thought appears to differ from affective thought in that it is directed, checked against feedback, evaluated in terms of its effectiveness in advancing specific goals, and protected from drift by deliberately controlled attention by the thinker [16].},
  isbn = {978-3-642-86679-1},
  langid = {english},
  keywords = {Conscious Thought,Pairwise Comparison,Priority Vector,Rank Preservation,Ratio Scale},
  file = {C:\Users\tcheng\Zotero\storage\J9JBSDY2\Saaty - 1987 - Principles of the Analytic Hierarchy Process.pdf}
}

@article{scheibehenneCanThereEver2010,
  title = {Can {{There Ever Be Too Many Options}}? {{A Meta-Analytic Review}} of {{Choice Overload}}},
  shorttitle = {Can {{There Ever Be Too Many Options}}?},
  author = {Scheibehenne, Benjamin and Greifeneder, Rainer and Todd, Peter M.},
  year = {2010},
  month = oct,
  journal = {Journal of Consumer Research},
  volume = {37},
  number = {3},
  pages = {409--425},
  issn = {0093-5301, 1537-5277},
  doi = {10.1086/651235},
  urldate = {2022-05-31},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\DUUA4SGJ\Scheibehenne et al. - 2010 - Can There Ever Be Too Many Options A Meta-Analyti.pdf}
}

@misc{ScratchVoteProceedings,
  title = {Scratch \& Vote {\textbar} {{Proceedings}} of the 5th {{ACM}} Workshop on {{Privacy}} in Electronic Society},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1179601.1179607},
  file = {C:\Users\tcheng\Zotero\storage\22DV5IS9\1179601.html}
}

@article{selkerMethodologyTestingVoting,
  title = {A {{Methodology}} for {{Testing Voting Systems}}},
  author = {Selker, Ted and Rosenzweig, Elizabeth and Pandolfo, Anna},
  pages = {15},
  abstract = {This paper compares the relative merit in realistic versus lab style experiments for testing voting technology. By analyzing three voting experiments, we describe the value of realistic settings in showing the enormous challenges for voting process control and consistent voting experiences.},
  langid = {english},
  keywords = {audit trails,methods and tools,usability testing,user-centered design,voting systems,voting technology testing},
  file = {C:\Users\tcheng\Zotero\storage\78A5BW8X\Selker et al. - A Methodology for Testing Voting Systems.pdf}
}

@inproceedings{selkerVotingUserExperience2003,
  title = {Voting: {{User}} Experience, Technology and Practice},
  booktitle = {{{CHI}} '03 Extended Abstracts on Human Factors in Computing Systems},
  author = {Selker, Ted and Fischer, Eric A. and Bederson, Benjamin B. and Mccormack, Conny and Nass, Clifford},
  year = {2003},
  series = {{{CHI EA}} '03},
  pages = {700--701},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/765891.765937},
  abstract = {This panel brings together usability and voting experts to discuss voting user experience in American governmental elections. Technological improvements and voting debacles have made this a special time for improving voting user experience. Can technologists improve the confidence citizens have in the voting system? What are the roles of teaching materials, registration processes, ballot design, polling place practices, equipment manufacturer relationships, and other human computer interaction processes in elections? Voting officials and politicians are eager for improvements in voting.This panel includes speakers from government and the CHI community to present legislative perspective, usability evaluation approach, administrators' view and behavioral science's suggestions for voting interface evaluation, design and deployment.},
  isbn = {1-58113-637-4},
  keywords = {computer human interface,ergonomics,voting},
  file = {C:\Users\tcheng\Zotero\storage\342MUSS5\Selker - Voting User Experience, Technology and Practice.pdf}
}

@article{senathirajahClinicianDriverSeat2014,
  title = {The Clinician in the {{Driver}}'s {{Seat}}: {{Part}} 1 -- {{A}} Drag/Drop User-Composable Electronic Health Record Platform},
  shorttitle = {The Clinician in the {{Driver}}'s {{Seat}}},
  author = {Senathirajah, Yalini and Bakken, Suzanne and Kaufman, David},
  year = {2014},
  month = dec,
  journal = {Journal of Biomedical Informatics},
  volume = {52},
  pages = {165--176},
  issn = {15320464},
  doi = {10.1016/j.jbi.2014.09.002},
  urldate = {2022-05-31},
  abstract = {Creating electronic health records that support the uniquely complex and varied needs of healthcare presents formidable challenges. To address some of these challenges we created a new model for healthcare information systems, embodied in MedWISE,2 a widget-based highly configurable electronic health record (EHR) platform. Founded on the idea that providing clinician users with greater control of the EHR may result in greater fit to user needs and preferences, MedWISE allows drag/drop user configurations and the sharing of user-created elements such as custom laboratory result panels and user-created interface tabs.},
  langid = {english},
  keywords = {cognitive load,interface},
  file = {C:\Users\tcheng\Zotero\storage\EBLVN8Q5\Senathirajah et al. - 2014 - The clinician in the Driver’s Seat Part 1 – A dra.pdf}
}

@article{seppCognitiveLoadTheory2019,
  title = {Cognitive {{Load Theory}} and {{Human Movement}}: {{Towards}} an {{Integrated Model}} of {{Working Memory}}},
  shorttitle = {Cognitive {{Load Theory}} and {{Human Movement}}},
  author = {Sepp, Stoo and Howard, Steven J. and {Tindall-Ford}, Sharon and Agostinho, Shirley and Paas, Fred},
  year = {2019},
  month = jun,
  journal = {Educational Psychology Review},
  volume = {31},
  number = {2},
  pages = {293--317},
  issn = {1573-336X},
  doi = {10.1007/s10648-019-09461-9},
  urldate = {2022-08-07},
  abstract = {Cognitive load theory (CLT) applies what is known about human cognitive architecture to the study of learning and instruction, to generate insights into the characteristics and conditions of effective instruction and learning. Recent developments in CLT suggest that the human motor system plays an important role in cognition and learning; however, it is unclear whether models of working memory (WM) that are typically espoused by CLT researchers can reconcile these novel findings. For instance, often-cited WM models envision separate information processing systems---such as Baddeley and Hitch's (1974) multicomponent model of WM---as a means to interpret modality-specific findings, although possible interactions with the human motor system remain under-explained. In this article, we examine the viability of these models to theoretically integrate recent research findings regarding the human motor system, as well as their ability to explain established CLT effects and other findings. We argue, it is important to explore alternate models of WM that focus on a single and integrated control of attention system that is applied to visual, phonological, embodied, and other sensory and nonsensory information. An integrated model such as this may better account for individual differences in experience and expertise and, parsimoniously, explain both recent and historical CLT findings across domains. To advance this aim, we propose an integrated model of WM that envisions a common and finite attentional resource that can be distributed across multiple modalities. How attention is mobilized and distributed across domains is interdependent, co-reinforcing, and ever-changing based on learners' prior experience and their immediate cognitive demands. As a consequence, the distribution of attentional focus and WM resources will vary across individuals and tasks, depending on the nature of the specific task being performed; the neurological, developmental, and experiential abilities of the individual; and the current availability of internal and external cognitive resources.},
  langid = {english},
  keywords = {Attention,Cognitive load theory,Gesturing,Human movement,Learning,Working memory model},
  file = {C:\Users\tcheng\Zotero\storage\Y3HWAJME\Sepp et al. - 2019 - Cognitive Load Theory and Human Movement Towards .pdf}
}

@inproceedings{shneidermanDirectManipulationComprehensible1997,
  title = {Direct Manipulation for Comprehensible, Predictable and Controllable User Interfaces},
  booktitle = {Proceedings of the 2nd International Conference on {{Intelligent}} User Interfaces  - {{IUI}} '97},
  author = {Shneiderman, Ben},
  year = {1997},
  pages = {33--39},
  publisher = {ACM Press},
  address = {Orlando, Florida, United States},
  doi = {10.1145/238218.238281},
  urldate = {2022-07-26},
  isbn = {978-0-89791-839-8},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\LESISKRJ\Shneiderman - 1997 - Direct manipulation for comprehensible, predictabl.pdf}
}

@article{shneidermanDirectManipulationStep1983,
  title = {Direct {{Manipulation}}: {{A Step Beyond Programming Languages}}},
  shorttitle = {Direct {{Manipulation}}},
  author = {{Shneiderman}},
  year = {1983},
  month = aug,
  journal = {Computer},
  volume = {16},
  number = {8},
  pages = {57--69},
  issn = {0018-9162},
  doi = {10.1109/MC.1983.1654471},
  urldate = {2022-07-26},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\GYIDZXUV\Shneiderman - 1983 - Direct Manipulation A Step Beyond Programming Lan.pdf}
}

@article{spassovaTakingLoadOut,
  title = {Taking the {{Load}} out of {{Choice Overload}}: {{Strategies}} for {{Reducing Cognitive Difficulty}} in {{Choice}} from {{Extensive Assortments}}},
  author = {Spassova, Gergana},
  pages = {5},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\P3SF2ARJ\Spassova - Taking the Load out of Choice Overload Strategies.pdf}
}

@techreport{stewart2005caltech,
  title = {Caltech/{{MIT}} Voting Technology Project},
  author = {Stewart III, Charles},
  year = {2005},
  institution = {VTP Working Paper},
  file = {C:\Users\tcheng\Zotero\storage\9VHGCUXN\vtp_wp36.pdf}
}

@incollection{strackThinkingJudgingCommunicating1987,
  title = {Thinking, {{Judging}}, and {{Communicating}}: {{A Process Account}} of {{Context Effects}} in {{Attitude Surveys}}},
  shorttitle = {Thinking, {{Judging}}, and {{Communicating}}},
  booktitle = {Social {{Information Processing}} and {{Survey Methodology}}},
  author = {Strack, Fritz and Martin, Leonard L.},
  editor = {Hippler, Hans-J. and Schwarz, Norbert and Sudman, Seymour},
  year = {1987},
  series = {Recent {{Research}} in {{Psychology}}},
  pages = {123--148},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4798-2_7},
  urldate = {2023-08-20},
  abstract = {The central goal of asking questions in a survey is to obtain reliable information about characteristics of the respondent. Asking, and consequently answering questions, however, never occurs in a vacuum. Rather, it occurs in a specific social and cognitive context that may influence responses in undesired ways (e.g., Schuman \& Presser, 1981). Thus, a change in the answer to a particular question may not necessarily reflect an attitude change on the part of the respondent but simply may be the influence of a different context. Schuman, Presser, and Ludwig (1981), for example, found that divergent responses toward abortion, as measured in two consecutive surveys, were caused not by a change of opinion over time but by the presence or absence of a particular question before the target question.},
  isbn = {978-1-4612-4798-2},
  langid = {english},
  keywords = {Context Effect,Contrast Effect,Response Scale,Survey Question,Target Stimulus}
}

@inproceedings{summers2014making,
  title = {Making Voting Accessible: Designing Digital Ballot Marking for People with Low Literacy and Mild Cognitive Disabilities},
  booktitle = {2014 Electronic Voting Technology {{Workshop}}/{{Workshop}} on Trustworthy Elections ({{EVT}}/{{WOTE}} 14)},
  author = {Summers, Kathryn and Chisnell, Dana and Davies, Drew and Alton, Noel and McKeever, Megan},
  year = {2014}
}

@misc{teamTaiwanDigitalMinister,
  title = {Taiwan {{Digital Minister}} Highlights Country's Use of Technology to Bolster Democracy in {{FT}} Interview},
  author = {Team, Internet},
  journal = {Taipei Representative Office in the U.K. 駐英國台北代表處},
  urldate = {2024-06-13},
  howpublished = {https://www.roc-taiwan.org/uk\_en/post/6295.html},
  langid = {english}
}

@article{thurstoneRankOrderPsychophysical1931,
  title = {Rank Order as a Psycho-Physical Method.},
  author = {Thurstone, L. L.},
  year = {1931},
  month = jun,
  journal = {Journal of Experimental Psychology},
  volume = {14},
  number = {3},
  pages = {187--201},
  issn = {0022-1015},
  doi = {10.1037/h0070025},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\KGBU23WC\Thurstone - 1931 - Rank order as a psycho-physical method..pdf}
}

@phdthesis{timbrook2013comparison,
  title = {A Comparison of a Traditional Ranking Format to a Drag-and-Drop Format with Stacking},
  author = {Timbrook, Jerry P},
  year = {2013},
  school = {University of Dayton}
}

@article{timbrookMasterArtsPsychology,
  title = {Master of {{Arts}} in {{Psychology}}},
  author = {Timbrook, Jerry Parmilee},
  pages = {128},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\Y8YV4BR5\Timbrook - Master of Arts in Psychology.pdf}
}

@article{toddInfluenceDecisionAids1994,
  title = {The Influence of Decision Aids on Choice Strategies under Conditions of High Cognitive Load},
  author = {Todd, P.A. and Benbasat, I.},
  year = {1994},
  month = apr,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {24},
  number = {4},
  pages = {537--547},
  issn = {0018-9472, 2168-2909},
  doi = {10.1109/21.286376},
  urldate = {2022-05-31},
  abstract = {The objective of this paper is to extend and complement the work reported in the behavioral decision theory literature on the role of effort and accuracy in choice tasks by examining the role of computer based decision aids in reducing cognitive effort under task conditions where the decision maker experiences heavy information load. The central proposition of this paper is that specific features can be incorporated within a set of decision aids that willalter the effort required to implement a particular choice strategy relative to other strategies, and that this will influence strategy selection. In a laboratory experiment, subjects were given different decision aids to reduce the cognitive effort associated with different preferential choice strategies. In particular, the decision aids provided varying levels of support for the processing associated with either elimination by aspects (EBA) or additive difference (AD) strategies. The study examined changes in operators which represent the subcomponentsor building blocks of the strategies. A repeated measures design was utilized whereby a total of 32 subjects performed a 30 alternative apartment selection task over two trials. Analysis of the data was based on the coding of concurrent verbal protocols which described the subjects' problem solving strategies.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\MXLV3XV5\Todd and Benbasat - 1994 - The influence of decision aids on choice strategie.pdf}
}

@article{toepoelSlidersVisualAnalogue2018,
  title = {Sliders, Visual Analogue Scales, or Buttons: {{Influence}} of Formats and Scales in Mobile and Desktop Surveys},
  shorttitle = {Sliders, Visual Analogue Scales, or Buttons},
  author = {Toepoel, Vera and Funke, Frederik},
  year = {2018},
  month = apr,
  journal = {Mathematical Population Studies},
  volume = {25},
  number = {2},
  pages = {112--122},
  issn = {0889-8480, 1547-724X},
  doi = {10.1080/08898480.2018.1439245},
  urldate = {2022-08-01},
  abstract = {In an experiment dealing with the use of personal computer, tablet, or mobile, scale points (up to 5, 7, or 11) and response formats (bars or buttons) are varied to examine differences in mean scores and nonresponse. The total number of ``not applicable'' answers does not vary significantly. Personal computer has the lowest item nonresponse, followed by mobile and tablet, and a lower mean score than for mobile. Slider bars showed lower mean scores and more nonresponses than buttons, indicating that they are more prone to bias and difficult in use. Sider bars, which work with a drag-and-drop principle, perform worse than visual analogue scales working with a point-and-click principle and buttons. Five-point scales have more nonresponses than eleven-point scales. Respondents evaluate 11-point scales more positively than shorter scales.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\D382SCGU\Toepoel and Funke - 2018 - Sliders, visual analogue scales, or buttons Influ.pdf}
}

@article{toepoelSmileysStarsHearts2019,
  title = {Smileys, {{Stars}}, {{Hearts}}, {{Buttons}}, {{Tiles}} or {{Grids}}: {{Influence}} of {{Response Format}} on {{Substantive Response}}, {{Questionnaire Experience}} and {{Response Time}}},
  shorttitle = {Smileys, {{Stars}}, {{Hearts}}, {{Buttons}}, {{Tiles}} or {{Grids}}},
  author = {Toepoel, Vera and Vermeeren, Brenda and Metin, Baran},
  year = {2019},
  month = apr,
  journal = {Bulletin of Sociological Methodology/Bulletin de M{\'e}thodologie Sociologique},
  volume = {142},
  number = {1},
  pages = {57--74},
  issn = {0759-1063, 2070-2779},
  doi = {10.1177/0759106319834665},
  urldate = {2024-06-13},
  abstract = {Studies of the processes underlying question answering in surveys suggest that the choice of (layout for) response categories can have a significant effect on respondent answers. In recent years, the use of pictures, such as emojis or stars, is often used in online communication. It is unclear if pictorial answer categories can replace traditional verbal formats as measurement instruments in surveys. In this article we investigate different versions of a Likert-scale to see if they generate similar results and user experiences. Data comes from the non-probability based Flitspanel in the Netherlands. The hearts and stars designs received lower average scores compared to the other formats. Smileys produced average answer scores in line with traditional radio buttons. Respondents evaluated the smiley design most positively. Grid designs were evaluated more negatively. People wanting to compare survey outcomes should be aware of these effects and only compare results when similar response formats are used.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\C9562ACT\Toepoel et al. - 2019 - Smileys, Stars, Hearts, Buttons, Tiles or Grids I.pdf}
}

@article{townsendVisualPreferenceHeuristic,
  title = {The ``{{Visual Preference Heuristic}}'': {{The Influence}} of {{Visual}} versus {{Verbal Depiction}} on {{Assortment Processing}}, {{Perceived Variety}}, and {{Choice Overload}}},
  author = {Townsend, Claudia and Kahn, Barbara E},
  journal = {JOURNAL OF CONSUMER RESEARCH},
  pages = {23},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\W2MWSRTV\Townsend and Kahn - The “Visual Preference Heuristic” The Inﬂuence of.pdf}
}

@article{townsendVisualPreferenceHeuristic2014,
  title = {The ``{{Visual Preference Heuristic}}'': {{The Influence}} of {{Visual}} versus {{Verbal Depiction}} on {{Assortment Processing}}, {{Perceived Variety}}, and {{Choice Overload}}},
  shorttitle = {The ``{{Visual Preference Heuristic}}''},
  author = {Townsend, Claudia and Kahn, Barbara E.},
  year = {2014},
  month = feb,
  journal = {Journal of Consumer Research},
  volume = {40},
  number = {5},
  pages = {993--1015},
  issn = {0093-5301, 1537-5277},
  doi = {10.1086/673521},
  urldate = {2022-05-31},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\LNFHYYWT\Townsend and Kahn - 2014 - The “Visual Preference Heuristic” The Influence o.pdf}
}

@incollection{tullisChapter10Case2008,
  title = {Chapter 10 - {{Case Studies}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {237--287},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00010-8},
  urldate = {2022-01-14},
  abstract = {This chapter presents six case studies showing how other usability researchers and practitioners have used metrics in their work. These case studies were redesigning a website cheaply and quickly; usability evaluation of speech recognition IVR; redesign of the CDC.gov website; usability benchmarking: mobile music and video; measuring the effects of drug label design and similarity on pharmacists' performance; and making metrics matter. All of these case studies illustrate various steps followed while planning a usability studies. One of the case studies involved a mixed approach, whereas the other focused on discovering and prioritizing usability problems and developing recommendations to eliminate the problems or reduce their impact. Likewise, one of them focused on comparing usability objectives and identifying individual scenarios where users face the problems. In one case study, researchers used time, success, and ease of use perception data with respect to benchmark goals as their focus. One of the case studies used behavioral and physiological measures as its focus. And the last one used different usability metrics to study the importance of metrics.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\RMZUN6B4\\Tullis and Albert - 2008 - Chapter 10 - Case Studies.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\PNP2QU64\\B9780123735584000108.html}
}

@incollection{tullisChapterBehavioralPhysiological2008,
  title = {Chapter 7 - {{Behavioral}} and {{Physiological Metrics}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {167--189},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00007-8},
  urldate = {2022-01-14},
  abstract = {This chapter describes a variety of behavioral and physiological measures that might be helpful in usability testing as additional ways of learning about the users' experiences with a product and their reactions to it. Some of these can be detected by careful observation, and some require specialized equipment. A structured approach to collecting observational data both verbal and nonverbal during a usability test can be very helpful in subsequent analysis. Facial expressions that participants make during a usability test may give additional insight into what they are thinking and feeling beyond what they say. A trained observer can detect and categorize many of these expressions, but some are very fleeting and may require video analysis. Eye-tracking can be a significant benefit in many kinds of usability tests. Its key value can be in determining whether participants in a usability test even looked at a particular element of the interface. Most eye-tracking systems must detect the location of the participant's pupil and calculate its diameter to determine where he or she is looking. Participants' pupils tend to dilate with higher mental workload and with overall arousal. Skin conductance and heart rate can be used to detect parts of an interface that participants find particularly frustrating. But the technology readily available today for measuring these is too intrusive for normal usability testing.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\S53C3MHS\\Tullis and Albert - 2008 - Chapter 7 - Behavioral and Physiological Metrics.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\BC6J9HWV\\B9780123735584000078.html}
}

@incollection{tullisChapterCombinedComparative2008,
  title = {Chapter 8 - {{Combined}} and {{Comparative Metrics}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {191--210},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00008-X},
  urldate = {2022-01-14},
  abstract = {Usability data are building blocks and each piece of usability data can be used to create new metrics. There are two ways to derive new usability metrics from existing data, namely, by combining more than one metric into a single usability measure, and by comparing existing usability data to expert or ideal results. The easiest way to combine different metrics is to compare each data point to a target goal and represent one single metric based on the percentage of participants who achieved a combined set of goals. This method of combining metrics based on target goals can be used with any set of metrics. An alternative to combining different metrics to derive an overall usability score is to graphically present the results of the metrics in a summary chart, called a Usability Scorecard. The goal is to present the data from the usability test in such a way that overall trends and important aspects of the data can be easily detected, such as tasks that are particularly problematic for the participants. The best way to assess the results of a usability test is to compare those results to goals that were established before the test. These goals may be set at the task level or at an overall level. Goals can be set for any of the metrics, including task completion, task time, errors, and self-reported measures.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\945JZXWH\\Tullis and Albert - 2008 - Chapter 8 - Combined and Comparative Metrics.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\4SEWMUC3\\B978012373558400008X.html}
}

@incollection{tullisChapterIssuesBasedMetrics2008,
  title = {Chapter 5 - {{Issues-Based Metrics}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {99--121},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00005-4},
  urldate = {2022-01-14},
  abstract = {This chapter reviews some simple metrics around usability issues, different ways of identifying usability issues, prioritizing the importance of different types of issues, and factors one needs to think about when measuring usability issues. Measuring usability issues helps to answer some fundamental questions about how good the design is and where to focus resources to remedy the outstanding problems. The easiest way to identify usability issues is during an in-person lab study. The more one understands the domain, the easier it will be to spot the issues. The severity of an issue can be determined in several ways. Severity always should take into account the impact on the user experience. Additional factors, such as frequency of use, impact on the business, and persistence, may also be considered. Some common measurements for usability issues are the frequency of unique issues, the percentage of participants who experience a specific issue, and the frequency of issues for different tasks or categories of issue. Additional analysis can be performed on high-severity issues or on how issues change from one design iteration to another. While identifying usability issues, questions about consistency and bias may arise. Therefore, it's important to work collaboratively as a team, focusing on high-priority issues, and to understand how different sources of bias impact conclusions. Careful participant recruitment and investing in wide task coverage are more fruitful than increasing the number of users.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\SB9EFM8U\\Tullis and Albert - 2008 - Chapter 5 - Issues-Based Metrics.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\6XBAMH3J\\B9780123735584000054.html}
}

@incollection{tullisChapterPerformanceMetrics2008,
  title = {Chapter 4 - {{Performance Metrics}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {63--97},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00004-2},
  urldate = {2022-01-14},
  abstract = {Performance metrics are powerful tools to evaluate the usability of any product. Performance metrics are always based on participants' behavior rather than what they say. There are five general types of performance metrics. Task success metrics are used when one is interested in whether participants are able to complete tasks using the product; in whether a user is successful or not based on a strict set of criteria; or in defining different levels of success based on the degree of completion, the experience in finding an answer, or the quality of the answer given. Time-on-task is a common performance metric that measures how much time is required to complete a task. Errors are a useful measure based on the number of mistakes made while attempting to complete a task. Efficiency is a way of evaluating the amount of effort required to complete a task. Efficiency is often measured by the number of steps or actions required to complete a task or by the ratio of the task success rate to the average time per task. Learnability involves looking at how any efficiency metric changes over time and is useful in examining how and when participants reach proficiency in using a product. Performance metrics rely not only on user behaviors but also on the use of scenarios or tasks.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\7CJUBPXF\\Tullis and Albert - 2008 - Chapter 4 - Performance Metrics.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\RE5JVQQ6\\B9780123735584000042.html}
}

@incollection{tullisChapterSelfReportedMetrics2008,
  title = {Chapter 6 - {{Self-Reported Metrics}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {123--166},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00006-6},
  urldate = {2022-01-14},
  abstract = {Self-reported data gives the most important information about users' perception of the system and their interaction with it. The most efficient way to capture self-reported data in a usability test is with the rating scale. Many different techniques are available for getting usability metrics from self-reported data. The best times to collect self-reported data are at the end of each task and at the end of the entire session. Task-level data can help you identify areas that need improvement and session-level data can help you get a sense of overall usability. The main goal of ratings associated with each task is to give you some insight into which tasks the participants thought were the most difficult. One of the most common uses of self-reported metrics is as an overall measure of perceived usability that participants are asked to give after having completed their interactions with the product. The System Usability Scale (SUS) questionnaires are used in a number of usability studies that involve comparing different designs for accomplishing similar tasks. For measuring user satisfaction, using one of the online services of a live website is a good option. Appropriate use of open-ended questions and techniques like checking for awareness or comprehension after interacting with the product are suggested.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\B2YQAX6G\\Tullis and Albert - 2008 - Chapter 6 - Self-Reported Metrics.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\P2YTY423\\B9780123735584000066.html}
}

@incollection{tullisChapterSpecialTopics2008,
  title = {Chapter 9 - {{Special Topics}}},
  booktitle = {Measuring the {{User Experience}}},
  author = {Tullis, Tom and Albert, Bill},
  editor = {Tullis, Tom and Albert, Bill},
  year = {2008},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {211--236},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-12-373558-4.00009-1},
  urldate = {2022-01-14},
  abstract = {This chapter introduces a number of topics related to the measurement or analysis of usability data but not traditionally thought of as part of ``mainstream'' usability data. These include information from live data on a production website, data from card-sorting studies, data related to the accessibility of a website, the topic of Six Sigma and how it relates to usability, and usability Return on Investment (ROI). The primary goal of the chapter is to make people aware of these topics, provide an overview of each of them, and then point to additional resources for more detailed information. If one is dealing with a live website, there's a potential treasure trove of data available about what the visitors are actually doing---what pages they're visiting, what links they're clicking on, and what paths they're following through the site. Card-sorting can be immensely helpful in learning how to organize some information or an entire website. Accessibility usually refers to how effectively someone with disabilities can use a particular system, application, or website. The other metrics should be applied to measure the usability of any system for users with different types. The basic idea behind usability ROI is to calculate the financial benefit attributable to usability enhancements for a product, system, or website. The key is to identify the cost associated with the usability improvements and then compare those to the financial benefits.},
  isbn = {978-0-12-373558-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\JJB3DHXI\\Tullis and Albert - 2008 - Chapter 9 - Special Topics.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\TE3J9WYH\\B9780123735584000091.html}
}

@article{tullisComparisonQuestionnairesAssessing2006,
  title = {A {{Comparison}} of {{Questionnaires}} for {{Assessing Website Usability}}},
  author = {Tullis, Thomas and Stetson, Jacqueline},
  year = {2006},
  month = jun,
  abstract = {Five questionnaires for assessing the usability of a website were compared in a study with 123 participants. The questionnaires studied were SUS, QUIS, CSUQ, a variant of Microsoft's Product Reaction Cards, and one that we have used in our Usability Lab for several years. Each participant performed two tasks on each of two websites: finance.yahoo.com and kiplinger.com. All five questionnaires revealed that one site was significantly preferred over the other. The data were analyzed to determine what the results would have been at different sample sizes from 6 to 14. At a sample size of 6, only 30-40\% of the samples would have identified that one of the sites was significantly preferred. Most of the data reach an apparent asymptote at a sample size of 12, where two of the questionnaires (SUS and CSUQ) yielded the same conclusion as the full dataset at least 90\% of the time.},
  file = {C:\Users\tcheng\Zotero\storage\GWEYYIWC\Tullis and Stetson - A Comparison of Questionnaires for Assessing Websi.pdf}
}

@inproceedings{turnbullRatingVotingRanking2007,
  title = {Rating, Voting \& Ranking: Designing for Collaboration \& Consensus},
  shorttitle = {Rating, Voting \& Ranking},
  booktitle = {{{CHI}} '07 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Turnbull, Don},
  year = {2007},
  month = apr,
  pages = {2705--2710},
  publisher = {ACM},
  address = {San Jose CA USA},
  doi = {10.1145/1240866.1241066},
  urldate = {2022-12-04},
  abstract = {The OpenChoice system, currently in development, is an open source, open access community rating and filtering service that would improve upon the utility of currently available Web content filters. The goal of OpenChoice is to encourage community involvement in making filtering classification more accurate and to increase awareness in the current approaches to content filtering. The design challenge for OpenChoice is to find the best interfaces for encouraging easy participation amongst a community of users, be it for voting, rating or discussing Web page content. This work in progress reviews some initial designs while reviewing best practices and designs from popular Web portals and community sites.},
  isbn = {978-1-59593-642-4},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\DSUM9QNR\\Turnbull - 2007 - Rating, voting & ranking designing for collaborat.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\LGSMZQTG\\Turnbull - 2007 - Rating, voting & ranking designing for collaborat.pdf}
}

@misc{UsabilityCodeVoting,
  title = {Usability of {{Code Voting Modalities}} {\textbar} {{Extended Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/3290607.3312971},
  file = {C:\Users\tcheng\Zotero\storage\7L3NQRUC\3290607.html}
}

@misc{UsabilityVotingSystems,
  title = {Usability of Voting Systems {\textbar} {{Proceedings}} of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1240624.1240653},
  file = {C:\Users\tcheng\Zotero\storage\2UBCTUKE\1240624.html}
}

@misc{UserResearchVoting,
  title = {User Research of a Voting Machine: Preliminary Findings and Experiences: {{Journal}} of {{Usability Studies}}: {{Vol}} 2, {{No}} 4},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.5555/2835552.2835555},
  file = {C:\Users\tcheng\Zotero\storage\EAAYMJ7I\2835552.html}
}

@misc{UVoteProceedings5th,
  title = {U-{{Vote}} {\textbar} {{Proceedings}} of the 5th {{Annual Workshop}} on {{Cyber Security}} and {{Information Intelligence Research}}: {{Cyber Security}} and {{Information Intelligence Challenges}} and {{Strategies}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1558607.1558665},
  file = {C:\Users\tcheng\Zotero\storage\XZZMZVJJ\1558607.html}
}

@misc{VerifiableAnonymousVote,
  title = {Verifiable Anonymous Vote Submission {\textbar} {{Proceedings}} of the 2008 {{ACM}} Symposium on {{Applied}} Computing},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1363686.1364202},
  file = {C:\Users\tcheng\Zotero\storage\UAUWIQAS\1363686.html}
}

@misc{VisualzationCognitiveLoad,
  title = {Visualzation, Cognitive Load - {{Google Search}}},
  urldate = {2023-12-15},
  howpublished = {https://www.google.com/search?q=visualzation\%2C+cognitive+load\&oq=visualzation\%2C+cognitive+load\&gs\_lcrp=EgZjaHJvbWUyBggAEEUYOTIICAEQABgWGB4yDQgCEAAYhgMYgAQYigUyDQgDEAAYhgMYgAQYigUyDQgEEAAYhgMYgAQYigUyDQgFEAAYhgMYgAQYigUyBggGEEUYQNIBCDQyOTNqMGoxqAIAsAIA\&sourceid=chrome\&ie=UTF-8\#ip=1}
}

@inproceedings{vonzezschwitzQuantifyingEffectivePassword2016,
  title = {On Quantifying the Effective Password Space of Grid-Based Unlock Gestures},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Mobile}} and {{Ubiquitous Multimedia}}},
  author = {{von Zezschwitz}, Emanuel and Eiband, Malin and Buschek, Daniel and Oberhuber, Sascha and De Luca, Alexander and Alt, Florian and Hussmann, Heinrich},
  year = {2016},
  month = dec,
  series = {{{MUM}} '16},
  pages = {201--212},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3012709.3012729},
  urldate = {2022-01-14},
  abstract = {We present a similarity metric for Android unlock patterns to quantify the effective password space of user-defined gestures. Our metric is the first of its kind to reflect that users choose patterns based on human intuition and interest in geometric properties of the resulting shapes. Applying our metric to a dataset of 506 user-defined patterns reveals very similar shapes that only differ by simple geometric transformations such as rotation. This shrinks the effective password space by 66\% and allows informed guessing attacks. Consequently, we present an approach to subtly nudge users to create more diverse patterns by showing background images and animations during pattern creation. Results from a user study (n = 496) show that applying such countermeasures can significantly increase pattern diversity. We conclude with implications for pattern choices and the design of enrollment processes.},
  isbn = {978-1-4503-4860-7},
  keywords = {metric,password space,security,similarity,unlock pattern,user selection},
  file = {C:\Users\tcheng\Zotero\storage\H6MH465L\von Zezschwitz et al. - 2016 - On quantifying the effective password space of gri.pdf}
}

@misc{VoteEarlyVote,
  title = {Vote {{Early}}, {{Vote Often}}: {{An}} e-Vote by Any Other Name?: {{Queue}}: {{Vol}} 2, {{No}} 6},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1028893.1028920},
  file = {C:\Users\tcheng\Zotero\storage\VMFHJEQD\1028893.html}
}

@misc{VoteMeProceedings,
  title = {Vote {{For Me}}! {\textbar} {{Proceedings}} of the 18th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.5555/3306127.3331955},
  file = {C:\Users\tcheng\Zotero\storage\T2ZT5HYU\3306127.html}
}

@misc{VoteographCHI10,
  title = {Vote-o-Graph {\textbar} {{CHI}} '10 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/1753846.1753959},
  file = {C:\Users\tcheng\Zotero\storage\Q2QGDMQK\1753846.html}
}

@misc{VotingVoteCapture,
  title = {Voting, Vote Capture \& Vote Counting Symposium {\textbar} {{Proceedings}} of the 2005 National Conference on {{Digital}} Government Research},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.5555/1065226.1065282},
  file = {C:\Users\tcheng\Zotero\storage\AA6LS4GX\1065226.html}
}

@article{wallaceComparisonTraditionalRankingTask,
  title = {A {{Comparison}} of a {{Traditional Ranking-Task}} and a},
  author = {Wallace, Linda},
  pages = {85},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\XK324MXU\Wallace - A Comparison of a Traditional Ranking-Task and a.pdf}
}

@article{wandButterflyDidIt2001,
  title = {The {{Butterfly Did It}}: {{The Aberrant Vote}} for {{Buchanan}} in {{Palm Beach County}}, {{Florida}}},
  shorttitle = {The {{Butterfly Did It}}},
  author = {Wand, Jonathan N. and Shotts, Kenneth W. and Sekhon, Jasjeet S. and Mebane, Walter R. and Herron, Michael C. and Brady, Henry E.},
  year = {2001},
  journal = {The American Political Science Review},
  volume = {95},
  number = {4},
  eprint = {3117714},
  eprinttype = {jstor},
  pages = {793--810},
  publisher = {[American Political Science Association, Cambridge University Press]},
  issn = {0003-0554},
  urldate = {2023-12-16},
  abstract = {We show that the butterfly ballot used in Palm Beach County, Florida, in the 2000 presidential election caused more than 2,000 Democratic voters to vote by mistake for Reform candidate Pat Buchanan, a number larger than George W. Bush's certified margin of victory in Florida. We use multiple methods and several kinds of data to rule out alternative explanations for the votes Buchanan received in Palm Beach County. Among 3,053 U.S. counties where Buchanan was on the ballot, Palm Beach County has the most anomalous excess of votes for him. In Palm Beach County, Buchanan's proportion of the vote on election-day ballots is four times larger than his proportion on absentee (nonbutterfly) ballots, but Buchanan's proportion does not differ significantly between election-day and absentee ballots in any other Florida county. Unlike other Reform candidates in Palm Beach County, Buchanan tended to receive election-day votes in Democratic precincts and from individuals who voted for the Democratic U.S. Senate candidate. Robust estimation of overdispersed binomial regression models underpins much of the analysis.}
}

@article{weijtersEffectRatingScale2010,
  title = {The Effect of Rating Scale Format on Response Styles: {{The}} Number of Response Categories and Response Category Labels},
  shorttitle = {The Effect of Rating Scale Format on Response Styles},
  author = {Weijters, Bert and Cabooter, Elke and Schillewaert, Niels},
  year = {2010},
  month = sep,
  journal = {International Journal of Research in Marketing},
  volume = {27},
  number = {3},
  pages = {236--247},
  issn = {0167-8116},
  doi = {10.1016/j.ijresmar.2010.02.004},
  urldate = {2024-06-13},
  abstract = {Questionnaires using Likert-type rating scales are an important source of data in marketing research. Researchers use different rating scale formats with varying numbers of response categories and varying label formats (e.g., 7-point rating scales labeled at the endpoints, fully labeled 5-point scales, etc.) but have few guidelines when selecting a specific format. Drawing from the literature on response styles, we formulate hypotheses on the effect of the labeling of response categories and the number of response categories on the net acquiescence response style, extreme response style and misresponse to reversed items. We test the hypotheses in an online survey (N=1207) with eight experimental conditions and a follow-up study with two experimental conditions (N=226). We find evidence of strong effects of scale format on response distributions and misresponse to reversed items, and we formulate recommendations on the choice of a scale format.},
  keywords = {Number of response categories,Rating scale format,Response category labels,Response styles},
  file = {C:\Users\tcheng\Zotero\storage\N4922YFW\S0167811610000303.html}
}

@article{weijtersExtremityHorizontalVertical2021,
  title = {Extremity in Horizontal and Vertical {{Likert}} Scale Format Responses. {{Some}} Evidence on How Visual Distance between Response Categories Influences Extreme Responding},
  author = {Weijters, Bert and Millet, Kobe and Cabooter, Elke},
  year = {2021},
  month = mar,
  journal = {International Journal of Research in Marketing},
  volume = {38},
  number = {1},
  pages = {85--103},
  issn = {01678116},
  doi = {10.1016/j.ijresmar.2020.04.002},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\BX3Q3RD6\Weijters et al. - 2021 - Extremity in horizontal and vertical Likert scale .pdf}
}

@misc{WhatDidReally,
  title = {What {{Did I Really Vote For}}? {{On}} the {{Usability}} of {{Verifiable E-Voting Schemes}} {\textbar} {{Proceedings}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2022-01-14},
  howpublished = {https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/3173574.3173750},
  file = {C:\Users\tcheng\Zotero\storage\DFXHHNID\3173574.html}
}

@article{wickens1990proximity,
  title = {Proximity Compatibility and Information Display: {{Effects}} of Color, Space, and Objectness on Information Integration},
  author = {Wickens, Christopher D and Andre, Anthony D},
  year = {1990},
  journal = {Human factors},
  volume = {32},
  number = {1},
  pages = {61--77},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@article{wickens1995proximity,
  title = {The Proximity Compatibility Principle: {{Its}} Psychological Foundation and Relevance to Display Design},
  author = {Wickens, Christopher D and Carswell, C Melody},
  year = {1995},
  journal = {Human factors},
  volume = {37},
  number = {3},
  pages = {473--494},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@book{wilsonProceedings1996Academy2015,
  title = {Proceedings of the 1996 {{Academy}} of {{Marketing Science}} ({{AMS}}) {{Annual Conference}}: {{Phoenix}}, {{Arizona}}, {{May}} 29-{{June}} 1, 1996},
  shorttitle = {Proceedings of the 1996 {{Academy}} of {{Marketing Science}} ({{AMS}}) {{Annual Conference}}},
  author = {Wilson, Elizabeth J. and Hair, Joseph F. and Sartwell, Susan},
  year = {2015},
  series = {Developments in Marketing Science},
  publisher = {Springer},
  address = {Cham},
  collaborator = {{Academy of Marketing Science}},
  isbn = {978-3-319-13144-3},
  langid = {english},
  lccn = {658.8},
  file = {C:\Users\tcheng\Zotero\storage\8ND9AF89\Improving Value Ratings An Assessment of the Rank-Then-Rate Approach.pdf}
}

@article{xiaoTellMeYourself2020,
  title = {Tell {{Me About Yourself}}: {{Using}} an {{AI-Powered Chatbot}} to {{Conduct Conversational Surveys}} with {{Open-ended Questions}}},
  shorttitle = {Tell {{Me About Yourself}}},
  author = {Xiao, Ziang and Zhou, Michelle X. and Liao, Q. Vera and Mark, Gloria and Chi, Changyan and Chen, Wenxi and Yang, Huahai},
  year = {2020},
  month = jun,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {27},
  number = {3},
  pages = {15:1--15:37},
  issn = {1073-0516},
  doi = {10.1145/3381804},
  urldate = {2024-06-13},
  abstract = {The rise of increasingly more powerful chatbots offers a new way to collect information through conversational surveys, where a chatbot asks open-ended questions, interprets a user's free-text responses, and probes answers whenever needed. To investigate the effectiveness and limitations of such a chatbot in conducting surveys, we conducted a field study involving about 600 participants. In this study with mostly open-ended questions, half of the participants took a typical online survey on Qualtrics and the other half interacted with an AI-powered chatbot to complete a conversational survey. Our detailed analysis of over 5,200 free-text responses revealed that the chatbot drove a significantly higher level of participant engagement and elicited significantly better quality responses measured by Gricean Maxims in terms of their informativeness, relevance, specificity, and clarity. Based on our results, we discuss design implications for creating AI-powered chatbots to conduct effective surveys and beyond.},
  keywords = {chatbot,Conversational agent,open-ended questions,survey},
  file = {C:\Users\tcheng\Zotero\storage\RDCMQSQ3\Xiao et al. - 2020 - Tell Me About Yourself Using an AI-Powered Chatbo.pdf}
}

@article{yanFastTimesEasy2008,
  title = {Fast Times and Easy Questions: The Effects of Age, Experience and Question Complexity on Web Survey Response Times},
  shorttitle = {Fast Times and Easy Questions},
  author = {Yan, Ting and Tourangeau, Roger},
  year = {2008},
  journal = {Applied Cognitive Psychology},
  volume = {22},
  number = {1},
  pages = {51--68},
  issn = {1099-0720},
  doi = {10.1002/acp.1331},
  urldate = {2022-07-31},
  abstract = {This paper examines response times (RT) to survey questions. Cognitive psychologists have long relied on response times to study cognitive processes but response time data have only recently received attention from survey researchers. To date, most of the studies on response times in surveys have treated response times either as a predictor or as a proxy measure for some other variable (e.g. attitude accessibility) of greater interest. As a result, response times have not been the main focus of the research. Focusing on the nature and determinants of response times, this paper examines variables that affect how long it takes respondents to answer questions in web surveys. Using the survey response model proposed by Tourangeau, Rips, and Rasinski (2000), we include both item-level characteristics and respondent-level characteristics thought to affect response times in a two-level cross-classified model. Much of the time spent on processing the questions involves reading and interpreting them. The results from the cross-classified models indicate that response times are affected by question characteristics such as the total number of clauses and the number of words per clause that probably reflect reading times. In addition, response times are also affected by the number and type of answer categories, and the location of the question within the questionnaire, as well as respondent characteristics such as age, education and experience with the Internet and with completing web surveys. Aside from their fixed effects on response times, respondent-level characteristics (such as age) are shown to vary randomly over questions and effects of question-level characteristics (such as types of questions and response scales) vary randomly over respondents. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\VMI4RQBF\\Yan and Tourangeau - 2008 - Fast times and easy questions the effects of age,.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\NAR6IGJN\\acp.html}
}
