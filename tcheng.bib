@article{alwinMeasurementValuesSurveys1985,
  title = {The Measurement of Values in Surveys: {{A}} Comparison of Ratings and Rankings},
  author = {Alwin, Duane F and Krosnick, Jon A},
  year = {1985},
  journal = {Public Opinion Quarterly},
  volume = {49},
  number = {4},
  pages = {535--552},
  publisher = {{Oxford University Press}},
  file = {C:\Users\tcheng\Zotero\storage\LJ9Y7Q7P\Alwin and Krosnick - 1985 - The Measurement of Values in Surveys A Comparison.pdf}
}

@article{bassettiCivicbaseOpensourcePlatform2023,
  title = {Civicbase: {{An}} Open-Source Platform for Deploying {{Quadratic Voting}} for {{Survey Research}}},
  shorttitle = {Civicbase},
  author = {Bassetti, Madeline E. and Dias, Gustavo and Chen, Daniel L. and Mortoni, Alan and Das, Ritesh},
  year = {2023},
  journal = {AI Magazine},
  volume = {44},
  number = {3},
  pages = {263--273},
  issn = {2371-9621},
  doi = {10.1002/aaai.12103},
  urldate = {2023-12-14},
  abstract = {Civic engagement is increasingly becoming digital. The ubiquity of computing increases our technologically mediated interactions. Governments have instated various digitization efforts to harness these new facets of virtual life. What remains to be seen is if citizen political opinion, which can inform the inception and effectiveness of public policy, is being accurately captured. Civicbase is an open-source online platform that supports the application of Quadratic Voting Survey for Research (QVSR), a novel survey method. In this paper, we explore QVSR as an effective method for eliciting policy preferences, optimal survey design for prediction, Civicbase's functionalities and technology stack, and Personal AI, an emerging domain, and its relevance to modeling individual political preferences.},
  langid = {english},
  file = {C\:\\Users\\tcheng\\Zotero\\storage\\QVPXVVU8\\Bassetti et al. - 2023 - Civicbase An open-source platform for deploying Q.pdf;C\:\\Users\\tcheng\\Zotero\\storage\\5BRC99PZ\\aaai.html}
}

@incollection{brunkenMeasuringCognitiveLoad2010,
  title = {Measuring {{Cognitive Load}}},
  booktitle = {Cognitive {{Load Theory}}},
  author = {Br{\"u}nken, Roland and Seufert, Tina and Paas, Fred},
  editor = {Plass, Jan L. and Br{\"u}nken, Roland and Moreno, Roxana},
  year = {2010},
  pages = {181--202},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511844744.011},
  urldate = {2023-12-16},
  abstract = {THE PROBLEM OF COGNITIVE LOAD MEASUREMENT: WHAT ARE GOOD COGNITIVE LOAD INDICATORS?The previous chapters have outlined the basic theoretical assumptions for cognitive load theory (Chapter 2), described how cognitive load affects the process of schema acquisition (Chapter 3), and discussed the role that learners' individual differences play in the process of knowledge construction (Chapter 4). The central problem identified by Cognitive Load Theory (CLT) is that learning is impaired when the total amount of processing requirements exceeds the limited capacity of human working memory.In addition to the fundamental assumption that learning is a function of available cognitive resources, CLT makes some additional assumptions with respect to the relation among cognitive resources, demands, and learning. The first of these additional assumptions is that instructional design and/or methods may induce either a useful (germane) or a wasteful (extraneous) consumption of cognitive capacity. The second assumption is that the source of cognitive load can also vary depending on the complexity of the task to-be-solved (intrinsic cognitive load defined by element interactivity). There is a large body of empirical research supporting the assumptions of CLT by analyzing the relation between the factors influencing cognitive load and learning outcomes. For example, several empirically well-established instructional design principles (Mayer, 2005) were identified in that line of research, which are discussed in other chapters of this book (see Chapters 7, 8). However, can CLT's assumptions be verified directly?},
  isbn = {978-0-521-86023-9}
}

@article{cain2007review,
  title = {A Review of the Mental Workload Literature},
  author = {Cain, Brad},
  year = {2007},
  journal = {DTIC Document},
  publisher = {{Citeseer}},
  file = {C:\Users\tcheng\Zotero\storage\GDI2HTR7\ADA474193.pdf}
}

@article{cavailleWhoCaresMeasuring,
  title = {Who {{Cares}}? {{Measuring Preference Intensity}} in a {{Polarized Environment}}},
  author = {Cavaille, Charlotte and Chen, Daniel L},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\ANDAME6J\Cavaille and Chen - Who Cares Measuring Preference Intensity in a Pol.pdf}
}

@misc{CharityNavigatorAnimals2023,
  title = {Charity {{Navigator}}: {{Animals}}},
  shorttitle = {Charity {{Navigator}}},
  year = {2023},
  month = mar,
  urldate = {2023-12-16},
  howpublished = {https://web.archive.org/web/20230323213631/https://www.charitynavigator.org/index.cfm?bay=search.categories\&categoryid=1},
  file = {C:\Users\tcheng\Zotero\storage\3SJHW2IC\index.html}
}

@article{chengCanShowWhat2021,
  title = {"{{I}} Can Show What {{I}} Really like.": {{Eliciting Preferences}} via {{Quadratic Voting}}},
  shorttitle = {"{{I}} Can Show What {{I}} Really like."},
  author = {Cheng, Ti-Chung and Li, Tiffany and Chou, Yi-Hung and Karahalios, Karrie and Sundaram, Hari},
  year = {2021},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  pages = {1--43},
  doi = {10.1145/3449281},
  abstract = {Surveys are a common instrument to gauge self-reported opinions from the crowd for scholars in the CSCW community, the social sciences, and many other research areas. Researchers often use surveys to prioritize a subset of given options when there are resource constraints. Over the past century, researchers have developed a wide range of surveying techniques, including one of the most popular instruments, the Likert ordinal scale [49], to elicit individual preferences. However, the challenge to elicit accurate and rich self-reported responses with surveys in a resource-constrained context still persists today. In this study, we examine Quadratic Voting (QV), a voting mechanism powered by the affordances of a modern computer and straddles ratings and rankings approaches [64], as an alternative online survey technique.We argue that QV could elicit more accurate self-reported responses compared to the Likert scale when the goal is to understand relative preferences under resource constraints. We conducted two randomized controlled experiments on Amazon Mechanical Turk, one in the context of public opinion polling and the other in a human-computer interaction user study. Based on our Bayesian analysis results, a QV survey with a sufficient amount of voice credits, aligned significantly closer to participants' incentive-compatible behaviors than a Likert scale survey, with a medium to high effect size. In addition, we extended QV's application scenario from typical public policy and education research to a problem setting familiar to the CSCW community: a prototypical HCI user study. Our experiment results, QV survey design, and QV interface serve as a stepping stone for CSCW researchers to further explore this surveying methodology in their studies and encourage decision-makers from other communities to consider QV as a promising alternative.},
  file = {C:\Users\tcheng\Zotero\storage\XDAKB8NE\Cheng et al. - 2021 - I can show what I really like. Eliciting Prefer.pdf}
}

@article{chernev2015choice,
  title = {Choice Overload: {{A}} Conceptual Review and Meta-Analysis},
  author = {Chernev, Alexander and B{\"o}ckenholt, Ulf and Goodman, Joseph},
  year = {2015},
  journal = {Journal of Consumer Psychology},
  volume = {25},
  number = {2},
  pages = {333--358},
  publisher = {{Elsevier}}
}

@article{conradElectronicVotingEliminates2009,
  title = {Electronic Voting Eliminates Hanging Chads but Introduces New Usability Challenges},
  author = {Conrad, Frederick G. and Bederson, Benjamin B. and Lewis, Brian and Peytcheva, Emilia and Traugott, Michael W. and Hanmer, Michael J. and Herrnson, Paul S. and Niemi, Richard G.},
  year = {2009},
  month = jan,
  journal = {International Journal of Human-Computer Studies},
  volume = {67},
  number = {1},
  pages = {111--124},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2008.09.010},
  urldate = {2023-12-16},
  abstract = {The arrival of electronic voting has generated considerable controversy, mostly about its vulnerability to fraud. By comparison, virtually no attention has been given to its usability, i.e., voters' ability to vote as they intend, which was central to the controversy surrounding the 2000 US presidential election. Yet it is hard to imagine a domain of human{\textendash}computer interaction where usability has more impact on how democracy works. This article reports a laboratory investigation of the usability of six electronic voting systems chosen to represent the features of systems in current use and potentially in future use. The primary question was whether e-voting systems are sufficiently hard to use that voting accuracy and satisfaction are compromised. We observed that voters often seemed quite lost taking far more than the required number of actions to cast individual votes, especially when they ultimately voted inaccurately. Their satisfaction went down as their effort went up. And accuracy with some systems was disturbingly low. While many of these problems are easy to fix, manufacturers will need to adopt usability engineering practices that have vastly improved user interfaces throughout the software industry.},
  keywords = {Usability of e-voting,Usability of electronic voting,Voting interfaces}
}

@book{daniel2017thinking,
  title = {Thinking, Fast and Slow},
  author = {Daniel, Kahneman},
  year = {2017}
}

@inproceedings{dawkinsPrimeIIIInnovative2009,
  title = {Prime {{III}}: An Innovative Electronic Voting Interface},
  shorttitle = {Prime {{III}}},
  booktitle = {Proceedings of the 14th International Conference on {{Intelligent}} User Interfaces},
  author = {Dawkins, Shane{\'e} and Sullivan, Tony and Rogers, Greg and Cross, E. Vincent and Hamilton, Lauren and Gilbert, Juan E.},
  year = {2009},
  month = feb,
  series = {{{IUI}} '09},
  pages = {485--486},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1502650.1502727},
  urldate = {2023-04-27},
  abstract = {Voting technology today has not addressed the issues that disabled voters are confronted with at the polls. Because approximately 17\% of the voting population is disabled, their issues should be handled with a solution geared towards their needs. Disabled voters need to be able to cast their vote without the assistance of others. The Prime III multimodal voting system [2] addresses these issues. This demonstration will illustrate the use of the Prime III system, a virtual reality (VR) version (Prime V), and a similar version created using a voice user interface (VUI).},
  isbn = {978-1-60558-168-2},
  keywords = {e-voting,multimodal user interaction,universal access},
  file = {C:\Users\tcheng\Zotero\storage\QDM2DYUN\Dawkins et al. - 2009 - Prime III an innovative electronic voting interfa.pdf}
}

@book{engstrom2020politics,
  title = {The Politics of Ballot Design: How States Shape {{American}} Democracy},
  author = {Engstrom, Erik J and Roberts, Jason M},
  year = {2020},
  publisher = {{Cambridge University Press}}
}

@inproceedings{everettElectronicVotingMachines2008a,
  title = {Electronic Voting Machines versus Traditional Methods: Improved Preference, Similar Performance},
  shorttitle = {Electronic Voting Machines versus Traditional Methods},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Everett, Sarah P. and Greene, Kristen K. and Byrne, Michael D. and Wallach, Dan S. and Derr, Kyle and Sandler, Daniel and Torous, Ted},
  year = {2008},
  month = apr,
  series = {{{CHI}} '08},
  pages = {883--892},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1357054.1357195},
  urldate = {2022-10-13},
  abstract = {In the 2006 U.S. election, it was estimated that over 66 million people would be voting on direct recording electronic (DRE) systems in 34\% of the nation's counties [8]. Although these computer-based voting systems have been widely adopted, they have not been empirically proven to be more usable than their predecessors. The series of studies reported here compares usability data from a DRE with those from more traditional voting technologies (paper ballots, punch cards, and lever machines). Results indicate that there were little differences between the DRE and these older methods in efficiency or effectiveness. However, in terms of user satisfaction, the DRE was significantly better than the older methods. Paper ballots also perform well, but participants were much more satisfied with their experiences voting on the DRE. The disconnect between subjective and objective usability has potential policy ramifications.},
  isbn = {978-1-60558-011-1},
  keywords = {dre,electronic voting,preference,usability,voting},
  file = {C:\Users\tcheng\Zotero\storage\74AKM33C\Everett et al. - 2008 - Electronic voting machines versus traditional meth.pdf}
}

@article{featherMeasurementValuesEffects1973,
  title = {The Measurement of Values: {{Effects}} of Different Assessment Procedures},
  shorttitle = {The Measurement of Values},
  author = {Feather, N. T.},
  year = {1973},
  month = dec,
  journal = {Australian Journal of Psychology},
  volume = {25},
  number = {3},
  pages = {221--231},
  issn = {0004-9530, 1742-9536},
  doi = {10.1080/00049537308255849},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\7MTFFIXE\Feather - 1973 - The measurement of values Effects of different as.pdf}
}

@article{gaoMentalWorkloadMeasurement2013,
  title = {Mental Workload Measurement for Emergency Operating Procedures in Digital Nuclear Power Plants},
  author = {Gao, Qin and Wang, Yang and Song, Fei and Li, Zhizhong and Dong, Xiaolu},
  year = {2013},
  month = jul,
  journal = {Ergonomics},
  volume = {56},
  number = {7},
  pages = {1070--1085},
  publisher = {{Taylor \& Francis}},
  issn = {0014-0139},
  doi = {10.1080/00140139.2013.790483},
  urldate = {2023-12-16},
  abstract = {Mental workload is a major consideration for the design of emergency operation procedures (EOPs) in nuclear power plants. Continuous and objective measures are desired. This paper compares seven mental workload measurement methods (pupil size, blink rate, blink duration, heart rate variability, parasympathetic/sympathetic ratio, total power and (Goals, Operations, Methods, and Section Rules)-(Keystroke Level Model) GOMS-KLM-based workload index) with regard to sensitivity, validity and intrusiveness. Eighteen participants performed two computerised EOPs of different complexity levels, and mental workload measures were collected during the experiment. The results show that the blink rate is sensitive to both the difference in the overall task complexity and changes in peak complexity within EOPs, that the error rate is sensitive to the level of arousal and correlate to the step error rate and that blink duration increases over the task period in both low and high complexity EOPs. Cardiac measures were able to distinguish tasks with different overall complexity. The intrusiveness of the physiological instruments is acceptable. Finally, the six physiological measures were integrated using group method of data handling to predict perceived overall mental workload. Practitioner Summary: The study compared seven measures for evaluating the mental workload with emergency operation procedure in nuclear power plants. An experiment with simulated procedures was carried out, and the results show that eye response measures are useful for assessing temporal changes of workload whereas cardiac measures are useful for evaluating the overall workload.},
  pmid = {23654299},
  keywords = {blinks,HRV,mental workload,operation procedure,pupil dilation}
}

@article{gilbertAnomalyDetectionElectronic2013,
  title = {Anomaly Detection in Electronic Voting Systems},
  author = {Gilbert, Juan E. and Dunbar, Jerone and Ottley, Alvitta and Smotherman, John Mark},
  year = {2013},
  month = sep,
  journal = {Information Design Journal (IDJ)},
  volume = {20},
  number = {3},
  pages = {194--206},
  issn = {01425471},
  doi = {10.1075/idj.20.3.01gil},
  abstract = {Studies have shown that voting error remains a problem with Direct Recording Electronic (DRE) voting machines. DRE's have an advantage over other voting technologies by facilitating ballot verification through review screens. However, results from ballot verification studies have shown that no more than half of study participants notice review screen anomalies (Campbell \& Byrne, 2009). This research replicated previous studies on anomaly detection on review screens using a multimodal voting system called Prime III. The results suggest that Prime III facilitates ballot verification and effectively yielded a detection rate of 90\%, even without informing participants on the importance of ballot verification.},
  keywords = {Anomaly detection (Computer security),Ballots,DRE,Electronic voting,electronic voting anomalies,HCI) and User Interfaces,Prime III,Verification of computer systems,VoteBox. Information interfaces and presentation (e.g.,Voting machines}
}

@article{goelKnapsackVotingVoting,
  title = {Knapsack {{Voting}}: {{Voting}} Mechanisms for {{Participatory Budgeting}}},
  author = {Goel, Ashish and Krishnaswamy, Anilesh K and Sakshuwong, Sukolsak and Aitamurto, Tanja},
  abstract = {We address the question of aggregating the preferences of voters in the context of participatory budgeting. We scrutinize the voting method currently used in practice, underline its drawbacks, and introduce a novel scheme tailored to this setting, which we call ``Knapsack Voting''. We study its strategic properties - we show that it is strategy-proof under a natural model of utility (a dis-utility given by the 1 distance between the outcome and the true preference of the voter), and ``partially'' strategy-proof under general additive utilities. We extend Knapsack Voting to more general settings with revenues, deficits or surpluses, and prove a similar strategy-proofness result. To further demonstrate the applicability of our scheme, we discuss its implementation on the digital voting platform that we have deployed in partnership with the local government bodies in many cities across the nation. From voting data thus collected, we present empirical evidence that Knapsack Voting works well in practice.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\WSQDYDLC\Goel et al. - Knapsack Voting Voting mechanisms for Participato.pdf}
}

@article{goodZoomableUserInterfaces2002,
  title = {Zoomable {{User Interfaces}} as a {{Medium}} for {{Slide Show Presentations}}},
  author = {Good, Lance and Bederson, Benjamin B},
  year = {2002},
  month = mar,
  journal = {Information Visualization},
  volume = {1},
  number = {1},
  pages = {35--49},
  publisher = {{SAGE Publications}},
  issn = {1473-8716},
  doi = {10.1057/palgrave.ivs.9500004},
  urldate = {2023-12-16},
  abstract = {In this paper, the authors propose Zoomable User Interfaces as an alternative presentation medium to address several common presentation problems. Zoomable User Interfaces offer new techniques for managing multiple versions of a presentation, providing interactive presentation navigation, and distinguishing levels of detail. These zoomable presentations may also offer several cognitive benefits over their commercial slide show counterparts. The authors also introduce CounterPoint, a tool to simplify the creation and delivery of zoomable presentations, discuss the techniques they have used to make authoring and navigation manageable in the multidimensional space. Lastly, some of the visualization principles compiled by the authors for designing these types of presentations are presented.},
  langid = {english}
}

@inproceedings{haapalainenPsychophysiologicalMeasuresAssessing2010,
  title = {Psycho-Physiological Measures for Assessing Cognitive Load},
  booktitle = {Proceedings of the 12th {{ACM}} International Conference on {{Ubiquitous}} Computing},
  author = {Haapalainen, Eija and Kim, SeungJun and Forlizzi, Jodi F. and Dey, Anind K.},
  year = {2010},
  month = sep,
  pages = {301--310},
  publisher = {{ACM}},
  address = {{Copenhagen Denmark}},
  doi = {10.1145/1864349.1864395},
  urldate = {2023-12-16},
  isbn = {978-1-60558-843-8},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\V3PHZIAF\Haapalainen et al. - 2010 - Psycho-physiological measures for assessing cognit.pdf}
}

@incollection{hart1988development,
  title = {Development of {{NASA-TLX}} (Task Load Index): {{Results}} of Empirical and Theoretical Research},
  booktitle = {Advances in Psychology},
  author = {Hart, Sandra G and Staveland, Lowell E},
  year = {1988},
  volume = {52},
  pages = {139--183},
  publisher = {{Elsevier}}
}

@article{hartNasaTaskLoadIndex2006a,
  title = {Nasa-{{Task Load Index}} ({{NASA-TLX}}); 20 {{Years Later}}},
  author = {Hart, Sandra G.},
  year = {2006},
  month = oct,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {50},
  number = {9},
  pages = {904--908},
  publisher = {{SAGE Publications Inc}},
  issn = {2169-5067},
  doi = {10.1177/154193120605000909},
  urldate = {2023-12-16},
  abstract = {NASA-TLX is a multi-dimensional scale designed to obtain workload estimates from one or more operators while they are performing a task or immediately afterwards. The years of research that preceded subscale selection and the weighted averaging approach resulted in a tool that has proven to be reasonably easy to use and reliably sensitive to experimentally important manipulations over the past 20 years. Its use has spread far beyond its original application (aviation), focus (crew complement), and language (English). This survey of 550 studies in which NASA-TLX was used or reviewed was undertaken to provide a resource for a new generation of users. The goal was to summarize the environments in which it has been applied, the types of activities the raters performed, other variables that were measured that did (or did not) covary, methodological issues, and lessons learned},
  file = {C:\Users\tcheng\Zotero\storage\HV7F8HJB\Hart - 2006 - Nasa-Task Load Index (NASA-TLX); 20 Years Later.pdf}
}

@article{hauserIntensityMeasuresConsumer1980a,
  title = {Intensity {{Measures}} of {{Consumer Preference}}},
  author = {Hauser, John R. and Shugan, Steven M.},
  year = {1980},
  journal = {Operations Research},
  volume = {28},
  number = {2},
  eprint = {170448},
  eprinttype = {jstor},
  pages = {278--320},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  urldate = {2023-12-16},
  abstract = {To design successful new products and services, managers need to measure consumer preferences relative to product attributes. Many existing methods use ordinal measures. Intensity measures have the potential to provide more information per question, thus allowing more accurate models or fewer consumer questions (lower survey cost, less consumer wearout). To exploit this potential, researchers must be able to identify how consumers react to these questions and must be able to estimate intensity-based preference functions. This paper provides a general structure for using intensity measures for estimating consumer preference functions. Within the structure: (1) alternative measurement theories are reviewed, (2) axioms for developing testable implications of each theory are provided, (3) statistical tests to test these implications and distinguish which theory describes how consumers are using the intensity measures are developed, (4) functional forms appropriate for the preference functions implied by each theory are derived, and (5) procedures to estimate the parameters of these preference functions are provided. Based on these results, a practical procedure, implemented by an interactive computer package, to measure preference functions in a market research environment is developed. An empirical case illustrates how the statistical tests and estimation procedures are used to aid in the design of new telecommunications devices. Empirical results suggest the majority of consumers can provide intensity judgments. The intensity-based estimation procedures do better on several criteria than ordinal estimation.}
}

@article{herrnsonEVALUATIONMARYLANDNEW2003,
  title = {{{AN EVALUATION OF MARYLAND}}'{{S NEW VOTING MACHINE}}},
  author = {Herrnson, Paul S. and Bederson, Benjamin B.},
  year = {2003},
  month = jan,
  urldate = {2023-12-16},
  abstract = {Four counties in Maryland used new touch screen voting machines in the 2002 elections, replacing their mechanical lever and punch card voting systems with the AccuVote-TS touch screen voting machine manufactured by Diebold Election Systems. The Center for American Politics and Citizenship (CAPC) and the Human-Computer Interaction Lab (HCIL) at the University of Maryland conducted an exit poll in Montgomery and Prince George's counties to evaluate the performance of the new voting machines. In this second of two reports prepared by CAPC and HCIL on the new voting machines, we found that most voters like the new voting machines and trust them to accurately record their votes. However, a significant number of voters still have concerns about the new machines, many needed help using them, and some continue to report technical problems with the machines. Voters who do not frequently use computers or have not attended college had the most difficulty using the machines. Major Findings: * Seven percent of voters felt that the touch screen voting machine was not easy to use, compared to 93 percent who felt it was easy to use or held a neutral opinion. * Nine percent of voters did not trust the touch screen voting machine, compared to with 91 percent who did. Only 70 percent trusted the mechanical lever or punch card system they previously used. * Three percent of voters reported encountering technical problems with the new machines. * Nine percent of the voters asked for and 17 percent received assistance using the new machine. * More than one-quarter of the voters who use computers once a month or less received assistance using the voting machine. * One-third of voters who have not attended college received assistance using the voting machine. * Voters in Prince George's County found the election judges to be more helpful than did voters in Montgomery County. Four counties in Maryland used new touch screen voting machines in the 2002 elections. Alleghany, Dorchester, Montgomery, and Prince George's replaced their mechanical lever and punch card voting systems with the AccuVote-TS touch screen voting machine manufactured by Diebold Election Systems. All 24 of Maryland's counties will purchase AccuVote-TS voting machines by 2006. The University of Maryland conducted an exit poll in Montgomery and Prince George's Counties to assess the performance of the new voting machine. Our sample included 1,276 respondents from 22 precincts in the two counties. The response rate was 74.6 percent. (UMIACS-TR-2002-107) (HCIL-TR-2002-25)},
  langid = {american},
  file = {C:\Users\tcheng\Zotero\storage\UJZQG24L\Herrnson and Bederson - 2003 - AN EVALUATION OF MARYLAND'S NEW VOTING MACHINE.pdf}
}

@article{iyengarWhenChoiceDemotivating2000,
  title = {When Choice Is Demotivating: {{Can}} One Desire Too Much of a Good Thing?},
  author = {Iyengar, Sheena S and Lepper, Mark R},
  year = {2000},
  journal = {Journal of personality and social psychology},
  volume = {79},
  number = {6},
  pages = {995},
  publisher = {{American Psychological Association}}
}

@incollection{krosnick2018measurement,
  title = {The Measurement of Attitudes},
  booktitle = {The Handbook of Attitudes},
  author = {Krosnick, Jon A and Judd, Charles M and Wittenbrink, Bernd},
  year = {2018},
  pages = {45--105},
  publisher = {{Routledge}}
}

@inproceedings{lalley2018quadratic,
  title = {Quadratic Voting: {{How}} Mechanism Design Can Radicalize Democracy},
  booktitle = {{{AEA}} Papers and Proceedings},
  author = {Lalley, Steven P and Weyl, E Glen},
  year = {2018},
  volume = {108},
  pages = {33--37},
  file = {C:\Users\tcheng\Zotero\storage\EF8MERKV\Lalley and Weyl - 2018 - Quadratic Voting How Mechanism Design Can Radical.pdf}
}

@inproceedings{leeUniversalDesignBallot2016,
  title = {Universal {{Design Ballot Interfaces}} on {{Voting Performance}} and {{Satisfaction}} of {{Voters}} with and without {{Vision Loss}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lee, Seunghyun "Tina" and Liu, Yilin Elaine and Ruzic, Ljilja and Sanford, Jon},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {4861--4871},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2858036.2858567},
  urldate = {2023-12-15},
  abstract = {Voting is a glocalized event across countries, states and municipalities in which individuals of all abilities want to participate. To enable people with disabilities to participate accessible voting is typically implemented by adding assistive technologies to electronic voting machines to accommodate people with disabilities. To overcome the complexities and inequities in this practice, two interfaces, EZ Ballot, which uses a linear yes/no input system for all selections, and QUICK Ballot, which provides random access voting through direct selection, were designed to provide one system for all voters. This paper reports efficacy testing of both interfaces. The study demonstrated that voters with a range of visual abilities were able to use both ballots independently. While non-sighted voters made fewer errors on the linear ballot (EZ Ballot), partially-sighted and sighted voters completed the random access ballot (QUICK Ballot) in less time. In addition, a higher percentage of non-sighted participants preferred the linear ballot, and a higher percentage of sighted participants preferred the random ballot.},
  isbn = {978-1-4503-3362-7},
  keywords = {effectiveness,efficiency,subjective usability,universal design,voting},
  file = {C:\Users\tcheng\Zotero\storage\BR83QRVT\Lee et al. - 2016 - Universal Design Ballot Interfaces on Voting Perfo.pdf}
}

@article{miller1956magical,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information.},
  author = {Miller, George A},
  year = {1956},
  journal = {Psychological review},
  volume = {63},
  number = {2},
  pages = {81},
  publisher = {{American Psychological Association}}
}

@book{moroneyQuestionnaireDesignHow2019,
  title = {Questionnaire {{Design}}: {{How}} to {{Ask}} the {{Right Questions}} of the {{Right People}} at the {{Right Time}} to {{Get}} the {{Information You Need}}},
  shorttitle = {Questionnaire {{Design}}},
  author = {Moroney, William F. and Cameron, Joyce A.},
  year = {2019},
  month = feb,
  publisher = {{Human Factors and Ergonomics Society}},
  isbn = {978-0-945289-55-5},
  langid = {english}
}

@article{naylor2017first,
  title = {First Year Student Conceptions of Success: {{What}} Really Matters?},
  author = {Naylor, Ryan and others},
  year = {2017},
  journal = {Student Success},
  volume = {8},
  number = {2},
  pages = {9--19},
  publisher = {{Student Success Journal}}
}

@misc{NewWayVoting,
  title = {A {{New Way}} of {{Voting That Makes Zealotry Expensive}} - {{Bloomberg}}},
  urldate = {2023-12-16},
  howpublished = {https://www.bloomberg.com/news/articles/2019-05-01/a-new-way-of-voting-that-makes-zealotry-expensive}
}

@book{norman2013design,
  title = {The Design of Everyday Things},
  author = {Norman Donald, A},
  year = {2013},
  publisher = {{MIT Press}}
}

@inproceedings{palinkoEstimatingCognitiveLoad2010,
  title = {Estimating Cognitive Load Using Remote Eye Tracking in a Driving Simulator},
  booktitle = {Proceedings of the 2010 {{Symposium}} on {{Eye-Tracking Research}} \& {{Applications}} - {{ETRA}} '10},
  author = {Palinko, Oskar and Kun, Andrew L. and Shyrokov, Alexander and Heeman, Peter},
  year = {2010},
  pages = {141},
  publisher = {{ACM Press}},
  address = {{Austin, Texas}},
  doi = {10.1145/1743666.1743701},
  urldate = {2023-12-16},
  isbn = {978-1-60558-994-7},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\2BY5Y3NR\Palinko et al. - 2010 - Estimating cognitive load using remote eye trackin.pdf}
}

@book{posner2018radical,
  title = {Radical Markets: {{Uprooting}} Capitalism and Democracy for a Just Society},
  author = {Posner, Eric A and Weyl, E Glen},
  year = {2018},
  publisher = {{Princeton University Press}}
}

@misc{QuadraticVotingFrontend2022,
  title = {Quadratic {{Voting Frontend}}},
  year = {2022},
  month = jan,
  urldate = {2023-12-16},
  copyright = {MIT},
  howpublished = {Public Digital Innovation Space}
}

@article{quarfoot2017quadratic,
  title = {Quadratic Voting in the Wild: Real People, Real Votes},
  author = {Quarfoot, David and {von Kohorn}, Douglas and Slavin, Kevin and Sutherland, Rory and Goldstein, David and Konar, Ellen},
  year = {2017},
  journal = {Public Choice},
  volume = {172},
  number = {1-2},
  pages = {283--303},
  publisher = {{Springer}}
}

@article{rintoulVisualAnimatedResponse,
  title = {Visual and Animated Response Formats in Web Surveys: {{Do}} They Produce Better Data, or Is It All Just Fun and Games?},
  author = {Rintoul, Duncan},
  pages = {126},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\97T7RREP\Rintoul - Visual and animated response formats in web survey.pdf}
}

@article{rubioEvaluationSubjectiveMental2004,
  title = {Evaluation of {{Subjective Mental Workload}}: {{A Comparison}} of {{SWAT}}, {{NASA-TLX}}, and {{Workload Profile Methods}}},
  shorttitle = {Evaluation of {{Subjective Mental Workload}}},
  author = {Rubio, Susana and D{\'i}az, Eva and Mart{\'i}n, Jes{\'u}s and Puente, Jos{\'e} M.},
  year = {2004},
  journal = {Applied Psychology},
  volume = {53},
  number = {1},
  pages = {61--86},
  issn = {1464-0597},
  doi = {10.1111/j.1464-0597.2004.00161.x},
  urldate = {2023-12-16},
  abstract = {Cette recherche mesure plusieurs propri{\'e}t{\'e}s psychom{\'e}triques (l'ing{\'e}rence, la sensibilit{\'e}, la valeur diagnostique et la validit{\'e}) de trois instruments multidimensionnels de l'{\'e}valuation de la charge de travail subjective: le NASA Task Load Index (TLX), le Subjective Workload Assessment Technique (SWAT) et le Workload Profile (WP). Sujets ont r{\'e}alis{\'e} deux t{\^a}ches de laboratoire s{\'e}par{\'e}ment (t{\^a}ches simples) et simultan{\'e}ment (t{\^a}ches doubles). D'apr{\`e}s l'analyse de variance, les trois instruments ne pr{\'e}sentent pas de diff{\'e}rences au niveau de l'ing{\'e}rence, mais WP b{\'e}n{\'e}ficie d'une sensibilit{\'e} exceptionnelle aux manipulations des diff{\'e}rentes t{\^a}ches. On a fait appel {\`a} une analyse canonique discriminante pour appr{\'e}cier la valeur diagnostique de chacun des trois instruments. Les r{\'e}sultats de l'analyse ont prouv{\'e} que les trois {\'e}valuations multidimensionnelles avaient fourni une information diagnostique sur la nature des exigences des t{\^a}ches qui {\'e}tait coh{\'e}rente avec leur description a priori. Toutefois, la valeur diagnostique du WP s'est r{\'e}v{\'e}l{\'e}e nettement sup{\'e}rieure {\`a} celles du TLX ou du SWAT. Pour {\'e}valuer la validit{\'e} concurrente de chaque instrument avec la performance aux t{\^a}ches, on a calcul{\'e} les corr{\'e}lations de Pearson entre chaque performance et chaque mesure de la charge subjective. On a enfin calcul{\'e} les corr{\'e}lations de Pearson entre les trois mesures de charge subjective pour {\'e}valuer la validit{\'e} convergente des instruments. Les trois coefficients ont {\'e}t{\'e} positifs et proche du maximum, soulignant ainsi la forte validit{\'e} convergente des trois outils retenus pour cette recherche. On a aussi compar{\'e} les conditions d'application et l'acceptabilit{\'e} par les sujets. On mentionne pour terminer les implications pratiques de ces trois sortes d'{\'e}valuation. The present research evaluates several psychometric properties (intrusiveness, sensitivity, diagnosticity, and validity) of three multidimensional subjective workload assessment instruments: the NASA Task Load Index (TLX), the Subjective Workload Assessment Technique (SWAT), and the Workload Profile (WP). Subjects performed two laboratory tasks separately (single task) and simultaneously (dual task). The results of the ANOVAs performed showed that there are no differences with regard to the three instruments' intrusiveness, and that among the three subjective workload instruments WP has an outstanding sensitivity to the different task manipulations. To evaluate the diagnosticity of each of the three instruments canonical discriminant analysis was used, and this demonstrated that the three multidimensional ratings provided diagnostic information on the nature of tasks demands that was consistent with the a priori task characterisation. However, the diagnostic power of WP was clearly superior to that obtained using TLX or SWAT. Pearson correlations between each performance and each subjective workload measure were calculated to evaluate the concurrent validity of each instrument with task performance, and to assess the convergent validity of the instruments. The three coefficients were positive and near to one, showing the high convergent validity of the three instruments considered in this research. Implementation requirements and subject acceptability were also compared. Finally, practical implications on the three assessment approaches are mentioned.},
  langid = {english}
}

@incollection{saaty1987principles,
  title = {Principles of the Analytic Hierarchy Process},
  booktitle = {Expert Judgment and Expert Systems},
  author = {Saaty, Thomas L},
  year = {1987},
  pages = {27--73},
  publisher = {{Springer}}
}

@article{saaty2003magic,
  title = {Why the Magic Number Seven plus or Minus Two},
  author = {Saaty, Thomas L and Ozdemir, Mujgan S},
  year = {2003},
  journal = {Mathematical and computer modelling},
  volume = {38},
  number = {3-4},
  pages = {233--244},
  publisher = {{Elsevier}}
}

@book{saaty2013group,
  title = {Group Decision Making: Drawing out and Reconciling Differences},
  author = {Saaty, Thomas L and Peniwati, Kirti},
  year = {2013},
  publisher = {{RWS publications}}
}

@inproceedings{summers2014making,
  title = {Making Voting Accessible: Designing Digital Ballot Marking for People with Low Literacy and Mild Cognitive Disabilities},
  booktitle = {2014 Electronic Voting Technology {{Workshop}}/{{Workshop}} on Trustworthy Elections ({{EVT}}/{{WOTE}} 14)},
  author = {Summers, Kathryn and Chisnell, Dana and Davies, Drew and Alton, Noel and McKeever, Megan},
  year = {2014}
}

@phdthesis{timbrook2013comparison,
  title = {A Comparison of a Traditional Ranking Format to a Drag-and-Drop Format with Stacking},
  author = {Timbrook, Jerry P},
  year = {2013},
  school = {University of Dayton}
}

@article{toepoelSlidersVisualAnalogue2018,
  title = {Sliders, Visual Analogue Scales, or Buttons: {{Influence}} of Formats and Scales in Mobile and Desktop Surveys},
  shorttitle = {Sliders, Visual Analogue Scales, or Buttons},
  author = {Toepoel, Vera and Funke, Frederik},
  year = {2018},
  month = apr,
  journal = {Mathematical Population Studies},
  volume = {25},
  number = {2},
  pages = {112--122},
  issn = {0889-8480, 1547-724X},
  doi = {10.1080/08898480.2018.1439245},
  urldate = {2022-08-01},
  abstract = {In an experiment dealing with the use of personal computer, tablet, or mobile, scale points (up to 5, 7, or 11) and response formats (bars or buttons) are varied to examine differences in mean scores and nonresponse. The total number of ``not applicable'' answers does not vary significantly. Personal computer has the lowest item nonresponse, followed by mobile and tablet, and a lower mean score than for mobile. Slider bars showed lower mean scores and more nonresponses than buttons, indicating that they are more prone to bias and difficult in use. Sider bars, which work with a drag-and-drop principle, perform worse than visual analogue scales working with a point-and-click principle and buttons. Five-point scales have more nonresponses than eleven-point scales. Respondents evaluate 11-point scales more positively than shorter scales.},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\D382SCGU\Toepoel and Funke - 2018 - Sliders, visual analogue scales, or buttons Influ.pdf}
}

@article{wandButterflyDidIt2001,
  title = {The {{Butterfly Did It}}: {{The Aberrant Vote}} for {{Buchanan}} in {{Palm Beach County}}, {{Florida}}},
  shorttitle = {The {{Butterfly Did It}}},
  author = {Wand, Jonathan N. and Shotts, Kenneth W. and Sekhon, Jasjeet S. and Mebane, Walter R. and Herron, Michael C. and Brady, Henry E.},
  year = {2001},
  journal = {The American Political Science Review},
  volume = {95},
  number = {4},
  eprint = {3117714},
  eprinttype = {jstor},
  pages = {793--810},
  publisher = {{[American Political Science Association, Cambridge University Press]}},
  issn = {0003-0554},
  urldate = {2023-12-16},
  abstract = {We show that the butterfly ballot used in Palm Beach County, Florida, in the 2000 presidential election caused more than 2,000 Democratic voters to vote by mistake for Reform candidate Pat Buchanan, a number larger than George W. Bush's certified margin of victory in Florida. We use multiple methods and several kinds of data to rule out alternative explanations for the votes Buchanan received in Palm Beach County. Among 3,053 U.S. counties where Buchanan was on the ballot, Palm Beach County has the most anomalous excess of votes for him. In Palm Beach County, Buchanan's proportion of the vote on election-day ballots is four times larger than his proportion on absentee (nonbutterfly) ballots, but Buchanan's proportion does not differ significantly between election-day and absentee ballots in any other Florida county. Unlike other Reform candidates in Palm Beach County, Buchanan tended to receive election-day votes in Democratic precincts and from individuals who voted for the Democratic U.S. Senate candidate. Robust estimation of overdispersed binomial regression models underpins much of the analysis.}
}

@article{weijtersExtremityHorizontalVertical2021,
  title = {Extremity in Horizontal and Vertical {{Likert}} Scale Format Responses. {{Some}} Evidence on How Visual Distance between Response Categories Influences Extreme Responding},
  author = {Weijters, Bert and Millet, Kobe and Cabooter, Elke},
  year = {2021},
  month = mar,
  journal = {International Journal of Research in Marketing},
  volume = {38},
  number = {1},
  pages = {85--103},
  issn = {01678116},
  doi = {10.1016/j.ijresmar.2020.04.002},
  urldate = {2022-01-14},
  langid = {english},
  file = {C:\Users\tcheng\Zotero\storage\BX3Q3RD6\Weijters et al. - 2021 - Extremity in horizontal and vertical Likert scale .pdf}
}

@article{wickens1990proximity,
  title = {Proximity Compatibility and Information Display: {{Effects}} of Color, Space, and Objectness on Information Integration},
  author = {Wickens, Christopher D and Andre, Anthony D},
  year = {1990},
  journal = {Human factors},
  volume = {32},
  number = {1},
  pages = {61--77},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}}
}

@article{wickens1995proximity,
  title = {The Proximity Compatibility Principle: {{Its}} Psychological Foundation and Relevance to Display Design},
  author = {Wickens, Christopher D and Carswell, C Melody},
  year = {1995},
  journal = {Human factors},
  volume = {37},
  number = {3},
  pages = {473--494},
  publisher = {{SAGE Publications Sage CA: Los Angeles, CA}}
}
